{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries - Setting seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Manipulation libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Visualization libraries - EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing - Cross valdidation - Evaluation metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow/Keras library - Deep Learning models\n",
    "import tensorflow\n",
    "from keras.layers import SimpleRNN, LSTM, Conv1D, MaxPooling1D, Flatten, Concatenate, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import Input, Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting seed libraries\n",
    "import os\n",
    "import random\n",
    "from tensorflow.random import set_seed\n",
    "from keras.utils import set_random_seed\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed value to 42\n",
    "seed_value= 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tensorflow.random.set_seed(seed_value)\n",
    "session_conf = tensorflow.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tensorflow.compat.v1.Session(graph=tensorflow.compat.v1.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing - Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data\n",
    "data = pd.read_excel('default of credit card clients.xls',header=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic info about variables\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for initial preprocessing of the data:\n",
    "1) replace EDUCATION values 0, 5, 6 with 4 ('other' category) since they are not mentioned in the data description\n",
    "2) replace MARRIAGE value 0 with 3 ('other' category) as there is not a 0 category for marriage column on data description \n",
    "3) drop 'ID' column - useless\n",
    "4) rename target column to DEFAULT, rename PAY_0 to PAY_1 for consistency and more accurate variable names\n",
    "'''\n",
    "def initial_preprocessing(df):\n",
    "    print('EDUCATION values before preprocessing:\\n',df['EDUCATION'].value_counts())\n",
    "    df['EDUCATION'].replace([0,5,6],4,inplace=True)\n",
    "    print('EDUCATION values after preprocessing:\\n',df['EDUCATION'].value_counts())\n",
    "    print('MARRIAGE values before preprocessing:\\n',df['MARRIAGE'].value_counts())\n",
    "    df['MARRIAGE'].replace(0,3,inplace=True)\n",
    "    print('MARRIAGE values after preprocessing:\\n',df['MARRIAGE'].value_counts())\n",
    "    df.drop(columns='ID',inplace=True)\n",
    "    df.rename(columns={\"default payment next month\": \"DEFAULT\",\"PAY_0\": \"PAY_1\"},inplace=True)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform initial preprocessing\n",
    "data = initial_preprocessing(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the imbalance of the dataset\n",
    "data.DEFAULT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of defaulters vs non-defaulters\n",
    "default_counts = data['DEFAULT'].value_counts()\n",
    "total_samples = len(data)\n",
    "\n",
    "# Calculate percentages\n",
    "default_percent = (default_counts / total_samples) * 100\n",
    "non_default_percent = 100 - default_percent\n",
    "\n",
    "# Create bar plot \n",
    "plt.figure(figsize=(8, 8))\n",
    "bars = plt.bar(default_counts.index, default_counts.values, color=['lightskyblue', 'dodgerblue'])\n",
    "plt.ylabel('Count',fontsize=16)\n",
    "plt.title('Distribution of defaulters vs non-defaulters',fontsize=22)\n",
    "plt.xticks(default_counts.index, ['Non-Default', 'Default'],fontsize=18)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display count and percentages\n",
    "for index, count in enumerate(default_counts):\n",
    "    plt.text(index, count + 300, f'{count} ({default_percent[index]:.2f}%)', ha='center',fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of defaults and non-defaults by gender\n",
    "gender_counts = data.groupby(['SEX', 'DEFAULT']).size().unstack(fill_value=0)\n",
    "gender_percentages = gender_counts.div(gender_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 7)) \n",
    "\n",
    "# Create a bar plot\n",
    "gender_counts.plot(kind='bar', stacked=True, color=['lightblue', 'dodgerblue'], ax=ax)\n",
    "ax.set_xlabel('SEX', fontsize=16)\n",
    "ax.set_ylabel('Count', fontsize=16)\n",
    "ax.set_title('Counts of Defaults and Non-Defaults by Gender', fontsize=18, y=1.02)\n",
    "plt.xticks([0, 1], ['Male', 'Female'], rotation=0, fontsize=18)  \n",
    "plt.yticks(fontsize=14)\n",
    "ax.legend(labels=['Non-Default', 'Default'], fontsize=11)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "for i, (gender_count, gender_percentage) in enumerate(zip(gender_counts.values, gender_percentages.values)):\n",
    "    total_count = sum(gender_count)\n",
    "    default_percentage = gender_percentage[1]\n",
    "    ax.text(i, total_count / 2, f'Default: {default_percentage:.2f}%', \n",
    "            ha='center', va='center', color='black', fontsize=15)\n",
    "    ax.text(i, total_count + 160, f'Total: {total_count}',\n",
    "            ha='center', color='black', fontsize=14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of defaults and non-defaults by education level - plot\n",
    "education_counts = data.groupby(['EDUCATION', 'DEFAULT']).size().unstack(fill_value=0)\n",
    "education_percentages = education_counts.div(education_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Create a bar plot\n",
    "education_counts.plot(kind='bar', stacked=True, color=['lightblue', 'dodgerblue'], ax=ax)\n",
    "ax.set_xlabel('EDUCATION',fontsize=20)\n",
    "ax.set_ylabel('Count',fontsize=20)\n",
    "ax.set_title('Counts of Defaults and Non-Defaults by Education Level',fontsize=23, y=1.02)\n",
    "x_positions = range(len(education_counts))\n",
    "x_labels = ['Graduate School', 'University', 'High School', 'Others']\n",
    "ax.set_xticks(ticks=x_positions)\n",
    "ax.set_xticklabels(x_labels, rotation=0,fontsize=19)\n",
    "ax.legend(labels=['Non-Default', 'Default'],fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Annotations\n",
    "for i, (education_count, education_percentage) in enumerate(zip(education_counts.values[:-1], \n",
    "                                                                education_percentages.values[:-1])):\n",
    "    total_count = sum(education_count)\n",
    "    default_percentage = education_percentage[1]\n",
    "    ax.text(i, total_count / 2, f'Default: {default_percentage:.2f}%',\n",
    "            ha='center', va='center', color='black',fontsize=20)\n",
    "    ax.text(i, total_count + 180, f'Total: {total_count}',\n",
    "            ha='center', color='black',fontsize=20)\n",
    "\n",
    "others_total_count = sum(education_counts.values[-1])\n",
    "others_default_percentage = education_percentages[1][4]\n",
    "ax.text(len(x_positions) - 1, others_total_count + 700, f'Total: {others_total_count}',\n",
    "        ha='center', color='black',fontsize=20)\n",
    "ax.text(len(x_positions) - 1, others_total_count + 180, f'Default: {others_default_percentage:.2f}%',\n",
    "        ha='center', color='black',fontsize=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of defaults and non-defaults by education level - plot\n",
    "marital_counts = data.groupby(['MARRIAGE', 'DEFAULT']).size().unstack(fill_value=0)\n",
    "marital_percentages = marital_counts.div(marital_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Create a bar plot\n",
    "marital_counts.plot(kind='bar', stacked=True, color=['lightblue', 'dodgerblue'], ax=ax)\n",
    "ax.set_xlabel('MARRIAGE',fontsize=20)\n",
    "ax.set_ylabel('Count',fontsize=20)\n",
    "ax.set_title('Counts of Defaults and Non-Defaults by Marital Status',fontsize=23, y=1.02)\n",
    "x_positions = range(len(marital_counts))\n",
    "x_labels = ['Married', 'Single', 'Others']\n",
    "ax.set_xticks(ticks=x_positions)\n",
    "ax.set_xticklabels(x_labels, rotation=0,fontsize=19)\n",
    "ax.legend(labels=['Non-Default', 'Default'],fontsize=14)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Annotations\n",
    "for i, (marital_count, marital_percentage) in enumerate(zip(marital_counts.values[:-1], \n",
    "                                                            marital_percentages.values[:-1])):\n",
    "    total_count = sum(marital_count)\n",
    "    default_percentage = marital_percentage[1]\n",
    "    ax.text(i, total_count / 2, f'Default: {default_percentage:.2f}%',\n",
    "            ha='center', va='center', color='black',fontsize=20)\n",
    "    ax.text(i, total_count + 180, f'Total: {total_count}', \n",
    "            ha='center', color='black',fontsize=20)\n",
    "\n",
    "others_total_count = sum(marital_counts.values[-1])\n",
    "others_default_percentage = marital_percentages[1][3]\n",
    "ax.text(len(x_positions) - 1, others_total_count + 900, \n",
    "        f'Total: {others_total_count}', ha='center', color='black',fontsize=20)\n",
    "ax.text(len(x_positions) - 1, others_total_count + 180, \n",
    "        f'Default: {others_default_percentage:.2f}%', ha='center', color='black',fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearman's correlation matrix for temporal features\n",
    "temporal_features = data[[\"PAY_1\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\",\n",
    "                        \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\",\n",
    "                        \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]]\n",
    "\n",
    "\n",
    "correlation_matrix = temporal_features.corr(method=\"spearman\")\n",
    "mask = np.triu(np.ones(correlation_matrix.shape), k=1)\n",
    "# Create a heatmap for visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', center=0, linewidths=0.5,\n",
    "            annot_kws={\"size\": 8, \"ha\": \"center\"},\n",
    "            fmt=\".1f\", mask=mask)\n",
    "plt.title(\"Spearman's Rank Correlation Matrix for Temporal Features\",fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical columns\n",
    "numerical_cols = ['LIMIT_BAL', 'AGE','BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6',\n",
    "                  'PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for each numerical column\n",
    "fig, axes = plt.subplots(nrows=len(numerical_cols) // 2, ncols=2, figsize=(12, 6 * len(numerical_cols) // 2))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    sns.boxplot(data=data, x=data['DEFAULT'].map({0: 'No Default', 1: 'Default'}),\n",
    "                y=col, ax=axes[i], palette=['dodgerblue', 'lightskyblue'])\n",
    "    axes[i].set_title(f'Boxplot of {col} by Default Status', fontsize=16)\n",
    "    axes[i].set_xlabel('')  \n",
    "    axes[i].set_ylabel(col,fontsize=14)\n",
    "    axes[i].tick_params(axis='both', labelsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution plots for each numerical column with 2 subplots per row\n",
    "fig, axes = plt.subplots(nrows=len(numerical_cols) // 2, ncols=2, figsize=(12, 6 * (len(numerical_cols) // 2)))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    ax = sns.histplot(data=data, x=col, hue='DEFAULT', ax=axes[i],\n",
    "                      kde=True, element='step', palette=['lightskyblue', 'dodgerblue'])\n",
    "    ax.set_title(f'Distribution of {col} by Default Status', fontsize=16)\n",
    "    ax.set_xlabel(col, fontsize=14)\n",
    "    ax.set_ylabel('Frequency', fontsize=14)\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "    legend = ax.get_legend()\n",
    "    legend.set_title(None)\n",
    "    new_labels = {'0': 'No Default', '1': 'Default'}\n",
    "    for t, l in zip(legend.texts, new_labels.values()):\n",
    "        t.set_text(l)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit balance vs Proportion of defaults plot \n",
    "limitbal_counts = data.groupby('LIMIT_BAL')['DEFAULT'].count()\n",
    "limitbal_defaults = data.groupby('LIMIT_BAL')['DEFAULT'].sum()\n",
    "limitbal_default_percentages = (limitbal_defaults / limitbal_counts) * 100\n",
    "x = np.array(limitbal_defaults.index).reshape(-1, 1)\n",
    "\n",
    "# Fit linear regression line\n",
    "model = LinearRegression()\n",
    "model.fit(x, limitbal_default_percentages)\n",
    "predicted_y = model.predict(x)\n",
    "\n",
    "#Create plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(limitbal_defaults.index, limitbal_default_percentages.values, color='dodgerblue', label='Actual Data')\n",
    "plt.plot(x, predicted_y, color='orange', label='Trendline')\n",
    "plt.xlabel('LIMIT_BAL',fontsize=17)\n",
    "plt.ylabel('Proportion of Defaults (%)',fontsize=17)\n",
    "plt.title('Limit Balance vs Proportion of Defaults',fontsize=23)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age vs Proportion of defaults plot\n",
    "age_counts = data.groupby('AGE')['DEFAULT'].count()\n",
    "age_defaults = data.groupby('AGE')['DEFAULT'].sum()\n",
    "age_default_percentages = (age_defaults / age_counts) * 100\n",
    "x = np.array(age_defaults.index).reshape(-1, 1)\n",
    "\n",
    "# Fit linear regression line\n",
    "model = LinearRegression()\n",
    "model.fit(x, age_default_percentages)\n",
    "predicted_y = model.predict(x)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(age_defaults.index, age_default_percentages.values, color='dodgerblue', label='Actual Data')\n",
    "plt.plot(x, predicted_y, color='orange', label='Trendline')\n",
    "plt.xlabel('AGE',fontsize=17)\n",
    "plt.ylabel('Proportion of Defaults (%)',fontsize=17)\n",
    "plt.title('Age vs Proportion of Defaults',fontsize=23)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- train test split (20% test) before scaling and encoding to prevent data leakages \n",
    "- train set will be used for cross-validation/hyperparameter tuning and test set for final evaluation \n",
    "- Stratify is used to ensure that the proportion of the class labels will remain consistent\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('DEFAULT',axis=1),data['DEFAULT'],\n",
    "                                                    test_size=0.2,stratify=data['DEFAULT'],random_state=42)\n",
    "print('X_train shape:',X_train.shape)\n",
    "print('X_test shape:',X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column transformer for Robust Scaler and One Hot Encoder \n",
    "# # making sure to return the preprocessed dataframes for better inspection\n",
    "\n",
    "class PreprocessorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns,columns_num, drop='first', handle_unknown='ignore',sparse_output=False):\n",
    "        self.columns = columns\n",
    "        self.columns_num = columns_num\n",
    "        self.drop = drop\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.sparse_output = sparse_output\n",
    "        self.encoders = {}\n",
    "        self.robust_enc = {}\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            encoder = OneHotEncoder(drop=self.drop, sparse_output=self.sparse_output, \n",
    "                                    handle_unknown=self.handle_unknown)\n",
    "            encoder.fit(X[[col]])\n",
    "            self.encoders[col] = encoder\n",
    "        \n",
    "        for col_num in self.columns_num:\n",
    "            encoder_robust = RobustScaler()\n",
    "            encoder_robust.fit(X[[col_num]])\n",
    "            self.robust_enc[col_num] = encoder_robust\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed = X.copy()\n",
    "        for col in self.columns:\n",
    "            encoder = self.encoders[col]\n",
    "            encoded_cols = encoder.transform(transformed[[col]])\n",
    "            new_cols = [f\"{col}_{value}\" for value in encoder.categories_[0][1:]]\n",
    "            encoded_cols_df = pd.DataFrame(encoded_cols, columns=new_cols, index=transformed.index)\n",
    "            transformed = pd.concat([transformed, encoded_cols_df], axis=1)\n",
    "        transformed = transformed.drop(self.columns, axis=1)\n",
    "        \n",
    "        for col_num in self.columns_num:\n",
    "            encoder_robust = self.robust_enc[col_num]\n",
    "            transformed[col_num] = encoder_robust.transform(transformed[[col_num]])\n",
    "            \n",
    "        return transformed\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    cat_cols = ['EDUCATION','MARRIAGE']\n",
    "    numerical_cols = ['LIMIT_BAL', 'AGE','BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5',\n",
    "                  'BILL_AMT6','PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "    \n",
    "    temp_cols = ['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5',\n",
    "    'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n",
    "    'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n",
    "    'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "    \n",
    "    PAY_cols = ['PAY_6', 'PAY_5', 'PAY_4', 'PAY_3', 'PAY_2','PAY_1']\n",
    "    BILL_AMT_cols = ['BILL_AMT6','BILL_AMT5','BILL_AMT4','BILL_AMT3','BILL_AMT2','BILL_AMT1']\n",
    "    PAY_AMT_cols = ['PAY_AMT6','PAY_AMT5', 'PAY_AMT4', 'PAY_AMT3', 'PAY_AMT2', 'PAY_AMT1']\n",
    "    \n",
    "    #fitting the column transformer\n",
    "    enc = PreprocessorTransformer(columns = cat_cols, columns_num= numerical_cols,\n",
    "                                  drop='first',handle_unknown='ignore',sparse_output=False) \n",
    "    \n",
    "    X_train_preprocessed = enc.fit_transform(X_train)\n",
    "    X_test_preprocessed = enc.transform(X_test)\n",
    "    \n",
    "    static_cols_train = X_train_preprocessed.drop(temp_cols,axis=1).columns.to_list()\n",
    "    static_cols_test = X_test_preprocessed.drop(temp_cols,axis=1).columns.to_list()\n",
    "    \n",
    "    #separation of static and temporal features\n",
    "    X_train_temp = X_train_preprocessed[temp_cols]\n",
    "    X_train_static = X_train_preprocessed[static_cols_train]\n",
    "    X_test_temp = X_test_preprocessed[temp_cols]\n",
    "    X_test_static = X_test_preprocessed[static_cols_test]\n",
    "\n",
    "    PAY_train = X_train_temp[PAY_cols].to_numpy()\n",
    "    BILL_AMT_train = X_train_temp[BILL_AMT_cols].to_numpy()\n",
    "    PAY_AMT_train = X_train_temp[PAY_AMT_cols].to_numpy()\n",
    "    \n",
    "    PAY_test = X_test_temp[PAY_cols].to_numpy()\n",
    "    BILL_AMT_test = X_test_temp[BILL_AMT_cols].to_numpy()\n",
    "    PAY_AMT_test = X_test_temp[PAY_AMT_cols].to_numpy()   \n",
    "    \n",
    "    # Stacking temporal features\n",
    "    stacked_train = np.dstack((PAY_train, BILL_AMT_train, PAY_AMT_train))\n",
    "    stacked_test = np.dstack((PAY_test, BILL_AMT_test, PAY_AMT_test))\n",
    "    y_train_preprocessed = y_train.to_numpy()\n",
    "    y_test_preprocessed = y_test.to_numpy()\n",
    "    \n",
    "    return stacked_train, X_train_static, y_train_preprocessed, stacked_test, X_test_static, y_test_preprocessed\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "n_splits = 5 \n",
    "kf = StratifiedKFold(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to receive temporal features and make predictions with the ML model as FCL\n",
    "def classifier_prediction_temporal(X_train_temp, X_test_temp, y_train_prep,\n",
    "                                   model, feature_extractor_model, layer_name):\n",
    "    \n",
    "    # Extract features using the feature_extractor_model\n",
    "    extractor_model = Model(inputs=feature_extractor_model.input, \n",
    "                            outputs=feature_extractor_model.get_layer(name=layer_name).output)\n",
    "    customers_vector = extractor_model.predict(X_train_temp)\n",
    "    customers_test = extractor_model.predict(X_test_temp)\n",
    "    \n",
    "    # Reshape the extracted features\n",
    "    reshaped_customers_vector = customers_vector.reshape(customers_vector.shape[0], -1)\n",
    "    reshaped_customers_test = customers_test.reshape(customers_test.shape[0], -1) \n",
    "    \n",
    "    # Train the classification model with the extracted features\n",
    "    final_model = model\n",
    "    final_model.fit(reshaped_customers_vector, y_train_prep)\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    preds = final_model.predict(reshaped_customers_test)\n",
    "    preds_train = final_model.predict(reshaped_customers_vector)\n",
    "    \n",
    "    return preds, preds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to receive both static and temporal features and make predictions with the ML model as FCL\n",
    "def classifier_prediction(X_train_temp,X_test_temp,X_train_st,\n",
    "                          X_test_st,y_train_prep,y_test_prep,model, feature_extractor_model, layer_name):\n",
    "    \n",
    "    extractor_model = Model(inputs=feature_extractor_model.input, \n",
    "                            outputs=feature_extractor_model.get_layer(name=layer_name).output)\n",
    "    customers_vector = extractor_model.predict([X_train_temp,X_train_st])\n",
    "    customers_test = extractor_model.predict([X_test_temp,X_test_st])\n",
    "    reshaped_customers_vector = customers_vector.reshape(customers_vector.shape[0], -1)\n",
    "    reshaped_customers_test = customers_test.reshape(customers_test.shape[0], -1) \n",
    "    final_model = model\n",
    "    final_model.fit(reshaped_customers_vector, y_train_prep)\n",
    "    preds = final_model.predict(reshaped_customers_test)\n",
    "    preds_train = final_model.predict(reshaped_customers_vector)\n",
    "    \n",
    "    return preds, preds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "architectures = [\n",
    "    {'hidden_layers': 4, 'units_per_layer': 32},\n",
    "    {'hidden_layers': 3, 'units_per_layer': 128}    \n",
    "]\n",
    "\n",
    "best_architecture = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for architecture in architectures:\n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    AUC_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static, y_test_dl = \n",
    "        preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "        \n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "        \n",
    "        # Build the RNN model\n",
    "        model = Sequential()\n",
    "        model.add(SimpleRNN(architecture['units_per_layer'], return_sequences=True, \n",
    "                            input_shape=(num_time_steps, num_features)))\n",
    "        for _ in range(1, architecture['hidden_layers']):\n",
    "            model.add(SimpleRNN(architecture['units_per_layer'], return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        initial_learning_rate = 0.0001  # Initial learning rate\n",
    "        decay_rate = 0.1  # Decay rate\n",
    "        decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "        \n",
    "        \n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                      optimizer=Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])\n",
    "        \n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "    \n",
    "        batch_size = 64\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                            validation_data=(stacked_test, y_test_dl),\n",
    "                            shuffle=False,callbacks=[lr_scheduler])\n",
    "        \n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        y_pred_probs = model.predict(stacked_test)\n",
    "        y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "        \n",
    "        accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "        precision = precision_score(y_test_dl,y_pred)\n",
    "        recall = recall_score(y_test_dl,y_pred)\n",
    "        f1 =  f1_score(y_test_dl,y_pred)\n",
    "        AUC = roc_auc_score(y_test_dl,y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        AUC_scores.append(AUC)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "        \n",
    "    # Calculate the average F1 score for the current architecture\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    # Print the scores for the current architecture\n",
    "    print(f\"Architecture: {architecture}\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "    print()\n",
    "    \n",
    "    # Check if the current architecture has a higher average F1 score\n",
    "    # If so, update the best architecture and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_architecture = architecture\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best architecture\n",
    "print(\"Best Architecture:\")\n",
    "print(best_architecture)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "architectures = [\n",
    "    {'hidden_layers': 5, 'units_per_layer': 32},\n",
    "    {'hidden_layers': 6, 'units_per_layer': 32}\n",
    "]\n",
    "\n",
    "best_architecture = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for architecture in architectures:\n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    AUC_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static,\n",
    "        y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "        \n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "        \n",
    "        # Build the LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(architecture['units_per_layer'], return_sequences=True, \n",
    "                       input_shape=(num_time_steps, num_features)))\n",
    "        for _ in range(1, architecture['hidden_layers']):\n",
    "            model.add(LSTM(architecture['units_per_layer'], return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        initial_learning_rate = 0.0001  # Initial learning rate\n",
    "        decay_rate = 0.1  # Decay rate\n",
    "        decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "        \n",
    "        \n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate),\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "    \n",
    "        batch_size = 64\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size, \n",
    "                            validation_data=(stacked_test, y_test_dl),\n",
    "                            shuffle=False,callbacks=[lr_scheduler])\n",
    "        \n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        y_pred_probs = model.predict(stacked_test)\n",
    "        y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "        \n",
    "        accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "        precision = precision_score(y_test_dl,y_pred)\n",
    "        recall = recall_score(y_test_dl,y_pred)\n",
    "        f1 =  f1_score(y_test_dl,y_pred)\n",
    "        AUC = roc_auc_score(y_test_dl,y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        AUC_scores.append(AUC)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "        \n",
    "    # Calculate the average F1 score for the current architecture\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "    total_params = model.count_params()  \n",
    "\n",
    "    # Print the scores for the current architecture\n",
    "    print(f\"Architecture: {architecture}\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "    print('total params:', total_params)\n",
    "    print()\n",
    "    \n",
    "    # Check if the current architecture has a higher average F1 score\n",
    "    # If so, update the best architecture and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_architecture = architecture\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best architecture\n",
    "print(\"Best Architecture:\")\n",
    "print(best_architecture)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "\n",
    "first_layer_filters = [128,256]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for filter in first_layer_filters:\n",
    "    \n",
    "            \n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    AUC_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "        X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "        \n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "        \n",
    "        # Build the CNN model\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=filter, kernel_size=5, activation='relu',padding='same',\n",
    "                         input_shape=(num_time_steps, num_features)))\n",
    "        model.add(MaxPooling1D(pool_size=3))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        initial_learning_rate = 0.001  # Initial learning rate\n",
    "        decay_rate = 0.1  # Decay rate\n",
    "        decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                      optimizer=Adam(learning_rate = initial_learning_rate), metrics=['accuracy'])\n",
    "        \n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "        \n",
    "        batch_size = 64\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                            validation_data=(stacked_test, y_test_dl),\n",
    "                            shuffle=False,callbacks=[lr_scheduler])\n",
    "        \n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        y_pred_probs = model.predict(stacked_test)\n",
    "        y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "        \n",
    "        accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "        precision = precision_score(y_test_dl,y_pred)\n",
    "        recall = recall_score(y_test_dl,y_pred)\n",
    "        f1 =  f1_score(y_test_dl,y_pred)\n",
    "        AUC = roc_auc_score(y_test_dl,y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        AUC_scores.append(AUC)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "    \n",
    "    # Calculate the average F1 score for the current configuration\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "    total_params = model.count_params()\n",
    "\n",
    "    # Print the scores for the current configuration\n",
    "    print(f\"Configuration: filters=({filter})\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "    print(\"Total Trainable Parameters of CNN:\", total_params)\n",
    "    print()\n",
    "\n",
    "    # Check if the current configuration has a higher average F1 score\n",
    "    # If so, update the best configuration and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_configuration = (filter)\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration of filters:\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_rnn_temporal(class_model):\n",
    "   \n",
    "   \n",
    "#insert the respective architecture configurations to be tested   \n",
    "    architectures = [\n",
    "        {'hidden_layers': 4, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 4, 'units_per_layer': 16}\n",
    "        \n",
    "        ]\n",
    "\n",
    "    best_architecture = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for architecture in architectures:\n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        AUC_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, \n",
    "            stacked_test, X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, \n",
    "                                                                     X_test_dl, y_test_dl)\n",
    "        \n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "            \n",
    "            # Build the RNN model\n",
    "            model = Sequential()\n",
    "            model.add(SimpleRNN(architecture['units_per_layer'],\n",
    "                                return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "            for _ in range(1, architecture['hidden_layers']):\n",
    "                model.add(SimpleRNN(architecture['units_per_layer'], return_sequences=True))\n",
    "            model.add(Flatten(name='RNN_FLATTEN'))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            \n",
    "            initial_learning_rate = 0.0001  # Initial learning rate\n",
    "            decay_rate = 0.1  # Decay rate\n",
    "            decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "            \n",
    "            \n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate), \n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "        \n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "            # Train the model\n",
    "            history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(stacked_test, y_test_dl),\n",
    "                                shuffle=False,callbacks=[lr_scheduler])\n",
    "            \n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl,\n",
    "                                                   model = class_model, feature_extractor_model=model, \n",
    "                                                   layer_name='RNN_FLATTEN')[0]\n",
    "            \n",
    "            accuracy =  accuracy_score(y_test_dl,preds)\n",
    "            precision = precision_score(y_test_dl,preds)\n",
    "            recall = recall_score(y_test_dl,preds)\n",
    "            f1 =  f1_score(y_test_dl,preds)\n",
    "            AUC = roc_auc_score(y_test_dl,preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            AUC_scores.append(AUC)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "          \n",
    "        # Calculate the number of parameters for the current architecture\n",
    "        total_params = model.count_params()  \n",
    "            \n",
    "        # Calculate the average F1 score for the current architecture\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "        # Print the scores for the current architecture\n",
    "        print(f\"Architecture: {architecture}\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "        print(\"Total Trainable Parameters of RNN:\", total_params)\n",
    "        print()\n",
    "        \n",
    "        # Check if the current architecture has a higher average F1 score\n",
    "        # If so, update the best architecture and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_architecture = architecture\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best architecture\n",
    "    print(\"Best Architecture:\")\n",
    "    print(best_architecture)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal(LogistiRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal(RandomForest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_lstm_temporal(class_model):\n",
    "   \n",
    "   #insert the respective architecture configurations to be tested\n",
    "   \n",
    "    architectures = [\n",
    "        {'hidden_layers': 1, 'units_per_layer': 128},\n",
    "        {'hidden_layers': 2, 'units_per_layer': 64},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 64},\n",
    "        {'hidden_layers': 2, 'units_per_layer': 128},\n",
    "        {'hidden_layers': 2, 'units_per_layer': 256},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 128},\n",
    "        {'hidden_layers': 4, 'units_per_layer': 16},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 4, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 4, 'units_per_layer': 64},\n",
    "        {'hidden_layers': 5, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 6, 'units_per_layer': 32}\n",
    "    ]\n",
    "\n",
    "    best_architecture = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for architecture in architectures:\n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        AUC_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "            X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "            \n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "            \n",
    "            # Build the LSTM model\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(architecture['units_per_layer'],\n",
    "                           return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "            for _ in range(1, architecture['hidden_layers']):\n",
    "                model.add(LSTM(architecture['units_per_layer'], return_sequences=True))\n",
    "            model.add(Flatten(name='LSTM_FLATTEN'))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            \n",
    "            initial_learning_rate = 0.0001  # Initial learning rate\n",
    "            decay_rate = 0.1  # Decay rate\n",
    "            decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "            \n",
    "            \n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate), \n",
    "                          metrics=['accuracy'])\n",
    "            total_params = model.count_params()\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "        \n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "            # Train the model\n",
    "            history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(stacked_test, y_test_dl),\n",
    "                                shuffle=False,callbacks=[lr_scheduler])\n",
    "            \n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl,\n",
    "                                                   model = class_model, feature_extractor_model=model, \n",
    "                                                   layer_name='LSTM_FLATTEN')[0]\n",
    "            \n",
    "            accuracy =  accuracy_score(y_test_dl,preds)\n",
    "            precision = precision_score(y_test_dl,preds)\n",
    "            recall = recall_score(y_test_dl,preds)\n",
    "            f1 =  f1_score(y_test_dl,preds)\n",
    "            AUC = roc_auc_score(y_test_dl,preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            AUC_scores.append(AUC)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "            \n",
    "        # Calculate the average F1 score for the current architecture\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "        # Print the scores for the current architecture\n",
    "        print(f\"Architecture: {architecture}\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "        print(\"Total Trainable Parameters of LSTM:\", total_params)\n",
    "        print()\n",
    "        \n",
    "        # Check if the current architecture has a higher average F1 score\n",
    "        # If so, update the best architecture and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_architecture = architecture\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best architecture\n",
    "    print(\"Best Architecture:\")\n",
    "    print(best_architecture)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal(LogistiRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal(RandomForest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cnn_temporal(class_model):\n",
    "    \n",
    "#insert the respective architecture configurations to be tested\n",
    "    first_layer_filters = [64,128,256]\n",
    "\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for filter in first_layer_filters:\n",
    "        \n",
    "                \n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        AUC_scores = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "            X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "            \n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "            \n",
    "            # Build the CNN model\n",
    "            model = Sequential()\n",
    "            model.add(Conv1D(filters=filter, kernel_size=5, activation='relu', \n",
    "                             padding='same', input_shape=(num_time_steps, num_features)))\n",
    "            model.add(MaxPooling1D(pool_size=3))\n",
    "            model.add(Flatten(name='CNN_FLATTEN'))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            \n",
    "            initial_learning_rate = 0.001  # Initial learning rate\n",
    "            decay_rate = 0.1  # Decay rate\n",
    "            decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "            \n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate = initial_learning_rate), \n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "            \n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "            \n",
    "            # Train the model\n",
    "            history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size, \n",
    "                                validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "            \n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl, \n",
    "                                                   model = class_model, feature_extractor_model=model, \n",
    "                                                   layer_name='CNN_FLATTEN')[0]\n",
    "            \n",
    "            accuracy =  accuracy_score(y_test_dl,preds)\n",
    "            precision = precision_score(y_test_dl,preds)\n",
    "            recall = recall_score(y_test_dl,preds)\n",
    "            f1 =  f1_score(y_test_dl,preds)\n",
    "            AUC = roc_auc_score(y_test_dl,preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            AUC_scores.append(AUC)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "            \n",
    "        # Calculate the number of parameters for the current configuration\n",
    "        total_params = model.count_params()\n",
    "        # Calculate the average F1 score for the current configuration\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "        # Print the scores for the current configuration\n",
    "        print(f\"Configuration: filters=({filter})\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "        print(\"Total Trainable Parameters of CNN:\", total_params)\n",
    "        print()\n",
    "\n",
    "        # Check if the current configuration has a higher average F1 score\n",
    "        # If so, update the best configuration and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_configuration = (filter)\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Configuration of filters:\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal(LogistiRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal(RandomForest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static + Temporal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "\n",
    "architectures = [\n",
    "\n",
    "    {'hidden_layers': 3, 'units_per_layer': 64},\n",
    "    {'hidden_layers': 3, 'units_per_layer': 128},\n",
    "    {'hidden_layers': 4, 'units_per_layer': 16},\n",
    "    {'hidden_layers': 3, 'units_per_layer': 32},\n",
    "     {'hidden_layers': 4, 'units_per_layer': 32},\n",
    "     {'hidden_layers': 4, 'units_per_layer': 64},\n",
    "     {'hidden_layers': 5, 'units_per_layer': 32},\n",
    "     {'hidden_layers': 6, 'units_per_layer': 32},\n",
    "]\n",
    "\n",
    "best_architecture = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for architecture in architectures:\n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    AUC_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "        X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "        \n",
    "        # Input layers\n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "        temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "        static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "        # RNN layers\n",
    "        rnn_layer = SimpleRNN(architecture['units_per_layer'], \n",
    "                              return_sequences=True, name=f'RNN_LAYER_1')(temporal_input)\n",
    "        for i in range(1, architecture['hidden_layers']):\n",
    "            rnn_layer = SimpleRNN(architecture['units_per_layer'],\n",
    "                                  return_sequences=True, name=f'RNN_LAYER_{i + 1}')(rnn_layer)\n",
    "        rnn_layer = Flatten(name='FLATTEN')(rnn_layer)\n",
    "\n",
    "        # Concatenate RNN layer with static input\n",
    "        rnn_combined = Concatenate(axis=1, name='rnn_CONCAT')([rnn_layer, static_input])\n",
    "        output = Dense(1, activation='sigmoid', name='rnn_OUTPUT_LAYER')(rnn_combined)\n",
    "\n",
    "        model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "        initial_learning_rate = 0.0001\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = 20\n",
    "        batch_size = 64\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "\n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                epochs=epochs, batch_size=batch_size,\n",
    "                                                validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "        y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "        precision = precision_score(y_test_dl, y_pred)\n",
    "        recall = recall_score(y_test_dl, y_pred)\n",
    "        f1 = f1_score(y_test_dl, y_pred)\n",
    "        AUC = roc_auc_score(y_test_dl, y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        AUC_scores.append(AUC)\n",
    "\n",
    "        fold_no = fold_no + 1\n",
    "        \n",
    "    # Calculate the number of parameters for the current architecture\n",
    "    total_params = model.count_params()  \n",
    "    # Calculate the average F1 score for the current architecture\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    # Print the scores for the current architecture\n",
    "    print(f\"Architecture: {architecture['hidden_layers']} RNN layers, {architecture['units_per_layer']} units\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "    print(\"Total Trainable Parameters of RNN:\", total_params)\n",
    "    print()\n",
    "\n",
    "    # Check if the current architecture has a higher average F1 score\n",
    "    # If so, update the best architecture and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_architecture = architecture\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "# Print the best architecture\n",
    "print(\"Best Architecture:\")\n",
    "print(best_architecture)\n",
    "print(\"Best F1 Score:\", best_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "architectures = [\n",
    "    \n",
    "    {'hidden_layers': 2, 'units_per_layer': 256}\n",
    "\n",
    "]\n",
    "\n",
    "best_architecture = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for architecture in architectures:\n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    AUC_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "        X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "        \n",
    "        # Input layers\n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "        temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "        static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "        # LSTM layers\n",
    "        lstm_layer = LSTM(architecture['units_per_layer'], \n",
    "                          return_sequences=True, name=f'LSTM_LAYER_1')(temporal_input)\n",
    "        for i in range(1, architecture['hidden_layers']):\n",
    "            lstm_layer = LSTM(architecture['units_per_layer'], \n",
    "                              return_sequences=True, name=f'LSTM_LAYER_{i + 1}')(lstm_layer)\n",
    "        lstm_layer = Flatten(name='FLATTEN')(lstm_layer)\n",
    "\n",
    "        # Concatenate LSTM layer with static input\n",
    "        LSTM_combined = Concatenate(axis=1, name='LSTM_CONCAT')([lstm_layer, static_input])\n",
    "        output = Dense(1, activation='sigmoid', name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "\n",
    "        model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "        initial_learning_rate = 0.0001\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = 20\n",
    "        batch_size = 64\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "\n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                epochs=epochs, batch_size=batch_size,\n",
    "                                                validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "        y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "        precision = precision_score(y_test_dl, y_pred)\n",
    "        recall = recall_score(y_test_dl, y_pred)\n",
    "        f1 = f1_score(y_test_dl, y_pred)\n",
    "        AUC = roc_auc_score(y_test_dl, y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        AUC_scores.append(AUC)\n",
    "\n",
    "        fold_no = fold_no + 1\n",
    "        \n",
    "    # Calculate the number of parameters for the current architecture\n",
    "    total_params = model.count_params()  \n",
    "    # Calculate the average F1 score for the current architecture\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    # Print the scores for the current architecture\n",
    "    print(f\"Architecture: {architecture['hidden_layers']} LSTM layers, {architecture['units_per_layer']} units\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "    print(\"Total Trainable Parameters of LSTM:\", total_params)\n",
    "    print()\n",
    "\n",
    "    # Check if the current architecture has a higher average F1 score\n",
    "    # If so, update the best architecture and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_architecture = architecture\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "# Print the best architecture\n",
    "print(\"Best Architecture:\")\n",
    "print(best_architecture)\n",
    "print(\"Best F1 Score:\", best_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "\n",
    "first_layer_filters = [4,8]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for filter in first_layer_filters:\n",
    "    \n",
    "            \n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    AUC_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "        X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "\n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "        \n",
    "        temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "        static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "        \n",
    "        # CNN layers\n",
    "        cnn_layer = Conv1D(filters=filter, kernel_size=5, activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "        cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "        cnn_layer = Conv1D(filters=filter*2, kernel_size=3, activation='relu',padding='same', name=f'CNN_LAYER_2')(cnn_layer)\n",
    "        cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "        cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "\n",
    "        # Concatenate CNN layer with static input\n",
    "        cnn_combined = Concatenate(axis=1, name='cnn_CONCAT')([cnn_layer, static_input])\n",
    "        output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(cnn_combined)\n",
    "        \n",
    "        model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "        \n",
    "        initial_learning_rate = 0.001  # Initial learning rate\n",
    "        decay_rate = 0.1  # Decay rate\n",
    "        decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "        batch_size = 64\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate = initial_learning_rate), \n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "    \n",
    "        # Train the model\n",
    "        history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                epochs=epochs, batch_size=batch_size,\n",
    "                                                validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                shuffle=False, callbacks=[lr_scheduler])\n",
    "        \n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "        y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "        precision = precision_score(y_test_dl, y_pred)\n",
    "        recall = recall_score(y_test_dl, y_pred)\n",
    "        f1 = f1_score(y_test_dl, y_pred)\n",
    "        AUC = roc_auc_score(y_test_dl, y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        AUC_scores.append(AUC)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "    \n",
    "    # Calculate the average F1 score for the current configuration\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    # Print the scores for the current configuration\n",
    "    print(f\"Configuration: filters={filter, filter*2}\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "    print()\n",
    "\n",
    "    # Check if the current configuration has a higher average F1 score\n",
    "    # If so, update the best configuration and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_configuration = (filter, filter*2)\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration of filters:\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_rnn_concat(class_model):\n",
    "\n",
    "    #insert the respective architecture configurations to be tested\n",
    "    architectures = [\n",
    "  \n",
    "        {'hidden_layers': 2, 'units_per_layer': 8},\n",
    "        {'hidden_layers': 1, 'units_per_layer': 16},\n",
    "           {'hidden_layers': 3, 'units_per_layer': 8},\n",
    "        {'hidden_layers': 1, 'units_per_layer': 32},\n",
    "           {'hidden_layers': 2, 'units_per_layer': 16},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 16},\n",
    "             {'hidden_layers': 1, 'units_per_layer':64},\n",
    "        {'hidden_layers': 2, 'units_per_layer': 32},\n",
    "           {'hidden_layers': 1, 'units_per_layer': 256},\n",
    "        {'hidden_layers': 1, 'units_per_layer': 128},\n",
    "         {'hidden_layers': 2, 'units_per_layer':64}\n",
    "    ]\n",
    "\n",
    "    best_architecture = None\n",
    "    best_f1_score = 0.0\n",
    "    \n",
    "    for architecture in architectures:\n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        AUC_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "            X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "            \n",
    "            # Input layers\n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "            temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "            static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "            # RNN layers\n",
    "            rnn_layer = SimpleRNN(architecture['units_per_layer'],\n",
    "                                  return_sequences=True, name=f'RNN_LAYER_1')(temporal_input)\n",
    "            for i in range(1, architecture['hidden_layers']):\n",
    "                rnn_layer = SimpleRNN(architecture['units_per_layer'], \n",
    "                                      return_sequences=True, name=f'RNN_LAYER_{i + 1}')(rnn_layer)\n",
    "            rnn_layer = Flatten(name='FLATTEN')(rnn_layer)\n",
    "\n",
    "            # Concatenate RNN layer with static input\n",
    "            RNN_combined = Concatenate(axis=1, name='RNN_CONCAT')([rnn_layer, static_input])\n",
    "            output = Dense(1, activation='sigmoid', name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "\n",
    "            model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "            initial_learning_rate = 0.0001\n",
    "            decay_rate = 0.1\n",
    "            decay_steps = 20\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate),\n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "\n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "            history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                    epochs=epochs, batch_size=batch_size,\n",
    "                                                    validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                    shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction(stacked_train,stacked_test,X_train_static, X_test_static,\n",
    "                                          y_train_dl,y_test_dl,model = class_model, \n",
    "                                          feature_extractor_model = model, layer_name='RNN_CONCAT')[0]\n",
    "            \n",
    "            accuracy = accuracy_score(y_test_dl, preds)\n",
    "            precision = precision_score(y_test_dl, preds)\n",
    "            recall = recall_score(y_test_dl, preds)\n",
    "            f1 = f1_score(y_test_dl, preds)\n",
    "            AUC = roc_auc_score(y_test_dl, preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            AUC_scores.append(AUC)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "            \n",
    "        # Calculate the average F1 score for the current architecture\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "        # Print the scores for the current architecture\n",
    "        print(f\"Architecture: {architecture}\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "        print()\n",
    "        \n",
    "        # Check if the current architecture has a higher average F1 score\n",
    "        # If so, update the best architecture and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_architecture = architecture\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best architecture\n",
    "    print(\"Best Architecture:\")\n",
    "    print(best_architecture)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grid_lstm_concat(class_model):\n",
    "\n",
    "   #insert the respective architecture configurations to be tested\n",
    "    architectures = [\n",
    "        {'hidden_layers': 4, 'units_per_layer': 64},\n",
    "         {'hidden_layers': 5, 'units_per_layer': 32},\n",
    "          {'hidden_layers': 6, 'units_per_layer': 32}\n",
    "    ]\n",
    "\n",
    "    best_architecture = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for architecture in architectures:\n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        AUC_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static, \n",
    "            y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "            \n",
    "            # Input layers\n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "            temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "            static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "            # LSTM layers\n",
    "            lstm_layer = LSTM(architecture['units_per_layer'], \n",
    "                              return_sequences=True, name=f'LSTM_LAYER_1')(temporal_input)\n",
    "            for i in range(1, architecture['hidden_layers']):\n",
    "                lstm_layer = LSTM(architecture['units_per_layer'], \n",
    "                                  return_sequences=True, name=f'LSTM_LAYER_{i + 1}')(lstm_layer)\n",
    "            lstm_layer = Flatten(name='FLATTEN')(lstm_layer)\n",
    "\n",
    "            # Concatenate LSTM layer with static input\n",
    "            LSTM_combined = Concatenate(axis=1, name='LSTM_CONCAT')([lstm_layer, static_input])\n",
    "            output = Dense(1, activation='sigmoid', name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "\n",
    "            model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "            initial_learning_rate = 0.0001\n",
    "            decay_rate = 0.1\n",
    "            decay_steps = 20\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate), \n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "\n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "            history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                    epochs=epochs, batch_size=batch_size,\n",
    "                                                    validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                    shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction(stacked_train,stacked_test,X_train_static, X_test_static,\n",
    "                                          y_train_dl,y_test_dl,model = class_model, \n",
    "                                          feature_extractor_model = model, layer_name='LSTM_CONCAT')[0]\n",
    "            \n",
    "            accuracy = accuracy_score(y_test_dl, preds)\n",
    "            precision = precision_score(y_test_dl, preds)\n",
    "            recall = recall_score(y_test_dl, preds)\n",
    "            f1 = f1_score(y_test_dl, preds)\n",
    "            AUC = roc_auc_score(y_test_dl, preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            AUC_scores.append(AUC)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "            \n",
    "        # Calculate the average F1 score for the current architecture\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "        # Print the scores for the current architecture\n",
    "        print(f\"Architecture: {architecture}\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "        print()\n",
    "        \n",
    "        # Check if the current architecture has a higher average F1 score\n",
    "        # If so, update the best architecture and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_architecture = architecture\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best architecture\n",
    "    print(\"Best Architecture:\")\n",
    "    print(best_architecture)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cnn_concat(class_model):\n",
    "\n",
    "    first_layer_filters = [32,64,128,256]\n",
    "\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for filter in first_layer_filters:\n",
    "        \n",
    "                \n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        AUC_scores = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "            X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "            \n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "            \n",
    "            temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "            static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "            \n",
    "            # CNN layers\n",
    "            cnn_layer = Conv1D(filters=filter, kernel_size=5,\n",
    "                               activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "            cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "            cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "\n",
    "            # Concatenate CNN layer with static input\n",
    "            cnn_combined = Concatenate(axis=1, name='CNN_CONCAT')([cnn_layer, static_input])\n",
    "            output = Dense(1, activation='sigmoid', name='CNN_OUTPUT_LAYER')(cnn_combined)\n",
    "            \n",
    "            model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "            \n",
    "            initial_learning_rate = 0.001  # Initial learning rate\n",
    "            decay_rate = 0.1  # Decay rate\n",
    "            decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "            \n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate = initial_learning_rate), \n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            \n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "            \n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "            # Train the model\n",
    "            history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                    epochs=epochs, batch_size=batch_size,\n",
    "                                                    validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                    shuffle=False, callbacks=[lr_scheduler])\n",
    "            \n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction(stacked_train,stacked_test,X_train_static, X_test_static,\n",
    "                                          y_train_dl,y_test_dl,model = class_model, \n",
    "                                          feature_extractor_model = model, layer_name='CNN_CONCAT')[0]\n",
    "                \n",
    "            accuracy = accuracy_score(y_test_dl, preds)\n",
    "            precision = precision_score(y_test_dl, preds)\n",
    "            recall = recall_score(y_test_dl, preds)\n",
    "            f1 = f1_score(y_test_dl, preds)\n",
    "            AUC = roc_auc_score(y_test_dl, preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            AUC_scores.append(AUC)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "        \n",
    "        # Calculate the average F1 score for the current configuration\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "        total_params = model.count_params()\n",
    "\n",
    "        # Print the scores for the current configuration\n",
    "        print(f\"Configuration: filters={filter}\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "        print(\"Total Trainable Parameters of CNN:\", total_params)\n",
    "        print()\n",
    "\n",
    "        # Check if the current configuration has a higher average F1 score\n",
    "        # If so, update the best configuration and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_configuration = (filter)\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Configuration of filters:\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Curves for Optimal Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excel file contains all the tables, so I retrieve teh tables and then I plot the elbow curves \n",
    "# 1 file for RNN, LSTM and 1 file for CNN\n",
    "\n",
    "paths = [\"C:/Users/diama/Downloads/elbowplots_lstm_rnn.xlsx\", \"C:/Users/diama/Downloads/elbowplots_cnn.xlsx\"]\n",
    "\n",
    "for excel_file_path in paths:\n",
    "    \n",
    "    with pd.ExcelFile(excel_file_path) as xls:\n",
    "        sheet_names = xls.sheet_names\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "    \n",
    "        data_excel = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "        data_excel = data_excel.rename(columns={'F1 Score': 'F1_score'})\n",
    "        num_parameters = data_excel['Num_parameters']\n",
    "        f1_scores = data_excel['F1_score']\n",
    "            \n",
    "        # Create the elbow plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(num_parameters, f1_scores, marker='o')\n",
    "        \n",
    "        # Find the index of the best architecture\n",
    "        best_index = data_excel['F1_score'].idxmax()\n",
    "        \n",
    "        # Highlight the point of the best architecture with a red dot\n",
    "        plt.scatter(num_parameters[best_index], f1_scores[best_index], c='r', s=60)\n",
    "        \n",
    "            \n",
    "        # Mark the point with the highest F1 score\n",
    "        best_arch = data_excel.iloc[best_index]\n",
    "        \n",
    "        if \"cnn\" in excel_file_path:\n",
    "            plt.text(0.97, 0.03, f'Best: Layers={int(best_arch[\"Layers\"])}, Filters={best_arch[\"Filters\"]}',\n",
    "             transform=plt.gca().transAxes, color='black', fontsize=10,\n",
    "             verticalalignment='bottom', horizontalalignment='right')\n",
    "        else:\n",
    "            plt.text(0.97, 0.03, f'Best: Layers={int(best_arch[\"Layers\"])}, Units={int(best_arch[\"Units\"])}',\n",
    "                transform=plt.gca().transAxes, color='black', fontsize=10,\n",
    "                verticalalignment='bottom', horizontalalignment='right')\n",
    "            \n",
    "        plt.xlabel('Number of Parameters')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Elbow Plot for {sheet_name}: F1 Score vs Number of Parameters')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer, Learning Rate, Batch Size Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "initial_learning_rates = [0.00001]\n",
    "optimizers = ['rmsprop']\n",
    "batches = [32]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "            \n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "            AUC_scores = []\n",
    "        \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                \n",
    "                # Build the RNN model\n",
    "                model = Sequential()\n",
    "                model.add(SimpleRNN(32, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "                model.add(SimpleRNN(32, return_sequences=True))\n",
    "                model.add(SimpleRNN(32, return_sequences=True))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "                \n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                \n",
    "                \n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "            \n",
    "                batch_size = bs\n",
    "                \n",
    "                # Train the model\n",
    "                history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                    validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                \n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                \n",
    "                y_pred_probs = model.predict(stacked_test)\n",
    "                y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "                \n",
    "                accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "                precision = precision_score(y_test_dl,y_pred)\n",
    "                recall = recall_score(y_test_dl,y_pred)\n",
    "                f1 =  f1_score(y_test_dl,y_pred)\n",
    "                AUC = roc_auc_score(y_test_dl,y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "                AUC_scores.append(AUC)\n",
    "                \n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "            print()\n",
    "        \n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "initial_learning_rates = [0.001,0.00001]\n",
    "optimizers = ['adam']\n",
    "batches = [64]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "            \n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "            AUC_scores = []\n",
    "        \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static,\n",
    "                y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                \n",
    "                # Build the LSTM model\n",
    "                model = Sequential()\n",
    "                model.add(LSTM(64, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "                \n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                \n",
    "                \n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "            \n",
    "                batch_size = bs\n",
    "                \n",
    "                # Train the model\n",
    "                history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                    validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                \n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                \n",
    "                y_pred_probs = model.predict(stacked_test)\n",
    "                y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "                \n",
    "                accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "                precision = precision_score(y_test_dl,y_pred)\n",
    "                recall = recall_score(y_test_dl,y_pred)\n",
    "                f1 =  f1_score(y_test_dl,y_pred)\n",
    "                AUC = roc_auc_score(y_test_dl,y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "                AUC_scores.append(AUC)\n",
    "                \n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "            print()\n",
    "        \n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "initial_learning_rates = [0.001]\n",
    "optimizers = ['adam']\n",
    "batches = [64]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "            \n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "            AUC_scores = []\n",
    "        \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static, \n",
    "                y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                \n",
    "                # Build the CNN model\n",
    "                model = Sequential()\n",
    "                model.add(Conv1D(filters=4, kernel_size=5, \n",
    "                                 activation='relu',padding='same', input_shape=(num_time_steps, num_features)))\n",
    "                model.add(MaxPooling1D(pool_size=3))\n",
    "                model.add(Conv1D(filters=8, padding='same', kernel_size=3, activation='relu'))\n",
    "                model.add(MaxPooling1D(pool_size=2))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "                \n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                \n",
    "                \n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "            \n",
    "                batch_size = bs\n",
    "                \n",
    "                # Train the model\n",
    "                history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                    validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                \n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                \n",
    "                y_pred_probs = model.predict(stacked_test)\n",
    "                y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "                \n",
    "                accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "                precision = precision_score(y_test_dl,y_pred)\n",
    "                recall = recall_score(y_test_dl,y_pred)\n",
    "                f1 =  f1_score(y_test_dl,y_pred)\n",
    "                AUC = roc_auc_score(y_test_dl,y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "                AUC_scores.append(AUC)\n",
    "                \n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "            print()\n",
    "        \n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_rnn_temporal_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.001,0.0001,0.00001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [32]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "                AUC_scores = []\n",
    "                \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                    \n",
    "                    # Build the RNN model\n",
    "                    model = Sequential()\n",
    "                    model.add(SimpleRNN(32, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "                    model.add(Flatten(name='RNN_FLATTEN'))\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "                    \n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                    \n",
    "                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                    \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                \n",
    "                    batch_size = bs\n",
    "                \n",
    "                    # Train the model\n",
    "                    history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                        validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                    \n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl, \n",
    "                                                           model = class_model, feature_extractor_model=model, \n",
    "                                                           layer_name='RNN_FLATTEN')[0]\n",
    "                    \n",
    "                    accuracy =  accuracy_score(y_test_dl,preds)\n",
    "                    precision = precision_score(y_test_dl,preds)\n",
    "                    recall = recall_score(y_test_dl,preds)\n",
    "                    f1 =  f1_score(y_test_dl,preds)\n",
    "                    AUC = roc_auc_score(y_test_dl,preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    AUC_scores.append(AUC)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                \n",
    "                # Calculate the number of parameters for the current configuration\n",
    "                total_params = model.count_params()  \n",
    "                    \n",
    "                # Calculate the average F1 score for the current configuration\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "                print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "            \n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "            \n",
    "            # Print the best configuration\n",
    "            print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "            print(best_configuration)\n",
    "            print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal_optim(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_lstm_temporal_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.001, 0.00001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [64]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "                AUC_scores = []\n",
    "        \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "                    \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                    \n",
    "                    # Build the LSTM model\n",
    "                    model = Sequential()\n",
    "                    model.add(LSTM(16, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "                    model.add(Flatten(name='LSTM_FLATTEN'))\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "                    \n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                    \n",
    "                    \n",
    "                    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate),\n",
    "                                  metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                    \n",
    "                    batch_size = bs                \n",
    "                    \n",
    "                    # Train the model\n",
    "                    history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                        validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                    \n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl,\n",
    "                                                           model = class_model, feature_extractor_model=model,\n",
    "                                                           layer_name='LSTM_FLATTEN')[0]\n",
    "                    \n",
    "                    accuracy =  accuracy_score(y_test_dl,preds)\n",
    "                    precision = precision_score(y_test_dl,preds)\n",
    "                    recall = recall_score(y_test_dl,preds)\n",
    "                    f1 =  f1_score(y_test_dl,preds)\n",
    "                    AUC = roc_auc_score(y_test_dl,preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    AUC_scores.append(AUC)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                    \n",
    "                # Calculate the average F1 score for the current configuration\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "                print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "                print()\n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Architecture:\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal_optim(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cnn_temporal_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [64]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "                AUC_scores = []\n",
    "                \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                    \n",
    "                    # Build the CNN model\n",
    "                    model = Sequential()\n",
    "                    model.add(Conv1D(filters=16, kernel_size=5,\n",
    "                                     activation='relu',padding='same', input_shape=(num_time_steps, num_features)))\n",
    "                    model.add(MaxPooling1D(pool_size=3))\n",
    "                    model.add(Conv1D(filters=32, padding='same', kernel_size=3, activation='relu'))\n",
    "                    model.add(MaxPooling1D(pool_size=2))\n",
    "                    model.add(Flatten(name = 'CNN_FLATTEN'))\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "                    \n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                    \n",
    "                  \n",
    "                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                    \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                \n",
    "                    batch_size = bs\n",
    "                \n",
    "                    # Train the model\n",
    "                    history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                        validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                    \n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl, \n",
    "                                                           model = class_model, feature_extractor_model=model,\n",
    "                                                           layer_name='CNN_FLATTEN')[0]\n",
    "                    \n",
    "                    accuracy =  accuracy_score(y_test_dl,preds)\n",
    "                    precision = precision_score(y_test_dl,preds)\n",
    "                    recall = recall_score(y_test_dl,preds)\n",
    "                    f1 =  f1_score(y_test_dl,preds)\n",
    "                    AUC = roc_auc_score(y_test_dl,preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    AUC_scores.append(AUC)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                \n",
    "                # Calculate the number of parameters for the current configuration\n",
    "                total_params = model.count_params()  \n",
    "                    \n",
    "                # Calculate the average F1 score for the current configuration\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "                print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "            \n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "            \n",
    "            # Print the best configuration\n",
    "            print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "            print(best_configuration)\n",
    "            print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal_optim(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static + Temporal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "initial_learning_rates = [0.001, 0.0001, 0.00001]\n",
    "optimizers = ['rmsprop','adam']\n",
    "batches = [128]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "\n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "            AUC_scores = []\n",
    "    \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static,\n",
    "                y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                # Input layers\n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                # RNN layers\n",
    "                rnn_layer = SimpleRNN(64, return_sequences=True, name=f'RNN_LAYER_1')(temporal_input)\n",
    "                rnn_layer = SimpleRNN(64, return_sequences=True, name=f'RNN_LAYER_2')(rnn_layer)\n",
    "                rnn_layer = SimpleRNN(64, return_sequences=True, name=f'RNN_LAYER_3')(rnn_layer)\n",
    "                rnn_layer = Flatten(name='FLATTEN')(rnn_layer)\n",
    "\n",
    "                # Concatenate RNN layer with static input\n",
    "                rnn_combined = Concatenate(axis=1, name='rnn_CONCAT')([rnn_layer, static_input])\n",
    "                output = Dense(1, activation='sigmoid', name='RNN_OUTPUT_LAYER')(rnn_combined)\n",
    "\n",
    "                model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "                \n",
    "                batch_size = bs\n",
    "                history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                        epochs=epochs, batch_size=batch_size,\n",
    "                                                        validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                        shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                # Make predictions on the test set\n",
    "                y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "                y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "                accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "                precision = precision_score(y_test_dl, y_pred)\n",
    "                recall = recall_score(y_test_dl, y_pred)\n",
    "                f1 = f1_score(y_test_dl, y_pred)\n",
    "                AUC = roc_auc_score(y_test_dl, y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "                AUC_scores.append(AUC)\n",
    "\n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "\n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "\n",
    "initial_learning_rates = [0.001, 0.0001, 0.00001]\n",
    "optimizers = ['rmsprop']\n",
    "batches = [64]\n",
    "\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "\n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "            AUC_scores = []\n",
    "    \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "                X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                # Input layers\n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                # LSTM layers\n",
    "                lstm_layer = LSTM(64, return_sequences=True, name=f'LSTM_LAYER_1')(temporal_input)\n",
    "                lstm_layer = LSTM(64, return_sequences=True, name=f'LSTM_LAYER_2')(lstm_layer)\n",
    "                lstm_layer = LSTM(64, return_sequences=True, name=f'LSTM_LAYER_3')(lstm_layer)\n",
    "                lstm_layer = LSTM(64, return_sequences=True, name=f'LSTM_LAYER_4')(lstm_layer)\n",
    "                lstm_layer = Flatten(name='FLATTEN')(lstm_layer)\n",
    "\n",
    "                # Concatenate LSTM layer with static input\n",
    "                lstm_combined = Concatenate(axis=1, name='LSTM_CONCAT')([lstm_layer, static_input])\n",
    "                output = Dense(1, activation='sigmoid', name='LSTM_OUTPUT_LAYER')(lstm_combined)\n",
    "\n",
    "                model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "\n",
    "                batch_size = bs\n",
    "\n",
    "                history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                        epochs=epochs, batch_size=batch_size,\n",
    "                                                        validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                        shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                # Make predictions on the test set\n",
    "                y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "                y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "                accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "                precision = precision_score(y_test_dl, y_pred)\n",
    "                recall = recall_score(y_test_dl, y_pred)\n",
    "                f1 = f1_score(y_test_dl, y_pred)\n",
    "                AUC = roc_auc_score(y_test_dl, y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "                AUC_scores.append(AUC)\n",
    "\n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "\n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "\n",
    "initial_learning_rates = [0.001, 0.0001, 0.00001]\n",
    "optimizers = ['rmsprop','adam']\n",
    "batches = [128]\n",
    "\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "\n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "            AUC_scores = []\n",
    "    \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                # Input layers\n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                # CNN layers\n",
    "                cnn_layer = Conv1D(filters=256, kernel_size=5, \n",
    "                                   activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "                cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "                cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "\n",
    "                # Concatenate CNN layer with static input\n",
    "                cnn_combined = Concatenate(axis=1, name='cnn_CONCAT')([cnn_layer, static_input])\n",
    "                output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(cnn_combined)\n",
    "\n",
    "                model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "                \n",
    "                batch_size = bs\n",
    "                \n",
    "                # Train DL model\n",
    "                history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                        epochs=epochs, batch_size=batch_size,\n",
    "                                                        validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                        shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                # Make predictions on the test set\n",
    "                y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "                y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "                accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "                precision = precision_score(y_test_dl, y_pred)\n",
    "                recall = recall_score(y_test_dl, y_pred)\n",
    "                f1 = f1_score(y_test_dl, y_pred)\n",
    "                AUC = roc_auc_score(y_test_dl, y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "                AUC_scores.append(AUC)\n",
    "\n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "\n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_rnn_concat_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.0001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [128]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "                AUC_scores = []\n",
    "        \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "                    \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    # Input layers\n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                    temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                    static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                    # RNN layers\n",
    "                    rnn_layer = SimpleRNN(32, return_sequences=True, name=f'RNN_LAYER_1')(temporal_input)\n",
    "                    rnn_layer = SimpleRNN(32, return_sequences=True, name=f'RNN_LAYER_2')(rnn_layer)\n",
    "                    rnn_layer = Flatten(name='FLATTEN')(rnn_layer)\n",
    "\n",
    "                    # Concatenate RNN layer with static input\n",
    "                    RNN_combined = Concatenate(axis=1, name='RNN_CONCAT')([rnn_layer, static_input])\n",
    "                    output = Dense(1, activation='sigmoid', name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "\n",
    "                    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                    \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                    \n",
    "                    batch_size = bs\n",
    "\n",
    "                    #Train DL model\n",
    "                    history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                            epochs=epochs, batch_size=batch_size,\n",
    "                                                            validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                            shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction(stacked_train,stacked_test,X_train_static, X_test_static,\n",
    "                                                  y_train_dl,y_test_dl,model = class_model, \n",
    "                                                  feature_extractor_model = model, layer_name='RNN_CONCAT')[0]\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_test_dl, preds)\n",
    "                    precision = precision_score(y_test_dl, preds)\n",
    "                    recall = recall_score(y_test_dl, preds)\n",
    "                    f1 = f1_score(y_test_dl, preds)\n",
    "                    AUC = roc_auc_score(y_test_dl, preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    AUC_scores.append(AUC)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                    \n",
    "                # Calculate the average F1 score for the current configuration\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "             \n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "                print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat_optim(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_lstm_concat_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.001, 0.00001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [64]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "                AUC_scores = []\n",
    "        \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "                    \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    # Input layers\n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                    temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                    static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                    # LSTM layers\n",
    "                    lstm_layer = LSTM(128, return_sequences=True, name='LSTM_LAYER_1')(temporal_input)\n",
    "                    lstm_layer = LSTM(128, return_sequences=True, name='LSTM_LAYER_2')(lstm_layer)\n",
    "                    lstm_layer = Flatten(name='FLATTEN')(lstm_layer)\n",
    "\n",
    "                    # Concatenate LSTM layer with static input\n",
    "                    LSTM_combined = Concatenate(axis=1, name='LSTM_CONCAT')([lstm_layer, static_input])\n",
    "                    output = Dense(1, activation='sigmoid', name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "\n",
    "                    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                    \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                    \n",
    "                    batch_size = bs\n",
    "\n",
    "                    #Train DL model\n",
    "                    history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                            epochs=epochs, batch_size=batch_size,\n",
    "                                                            validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                            shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction(stacked_train,stacked_test,X_train_static,\n",
    "                                                  X_test_static,y_train_dl,y_test_dl,model = class_model, \n",
    "                                                  feature_extractor_model = model, layer_name='LSTM_CONCAT')[0]\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_test_dl, preds)\n",
    "                    precision = precision_score(y_test_dl, preds)\n",
    "                    recall = recall_score(y_test_dl, preds)\n",
    "                    f1 = f1_score(y_test_dl, preds)\n",
    "                    AUC = roc_auc_score(y_test_dl, preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    AUC_scores.append(AUC)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                    \n",
    "                # Calculate the average F1 score for the current architecture\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "             \n",
    "\n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "                print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat_optim(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cnn_concat_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [64]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "                AUC_scores = []\n",
    "        \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "                    \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    # Input layers\n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                    temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                    static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                    # CNN layers\n",
    "                    cnn_layer = Conv1D(filters=8, kernel_size=5,\n",
    "                                       activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "                    cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "                    cnn_layer = Conv1D(filters=16, kernel_size=3,\n",
    "                                       activation='relu',padding='same', name=f'CNN_LAYER_2')(cnn_layer)\n",
    "                    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "                    cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "\n",
    "                    # Concatenate CNN layer with static input\n",
    "                    cnn_combined = Concatenate(axis=1, name='CNN_CONCAT')([cnn_layer, static_input])\n",
    "                    output = Dense(1, activation='sigmoid', name='CNN_OUTPUT_LAYER')(cnn_combined)\n",
    "                    \n",
    "                    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                    \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                    \n",
    "                    batch_size = bs\n",
    "\n",
    "                    # Train DL model\n",
    "                    history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                            epochs=epochs, batch_size=batch_size,\n",
    "                                                            validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                            shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction(stacked_train,stacked_test,X_train_static,\n",
    "                                                  X_test_static,y_train_dl,y_test_dl,model = class_model,\n",
    "                                                  feature_extractor_model = model, layer_name='CNN_CONCAT')[0]\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_test_dl, preds)\n",
    "                    precision = precision_score(y_test_dl, preds)\n",
    "                    recall = recall_score(y_test_dl, preds)\n",
    "                    f1 = f1_score(y_test_dl, preds)\n",
    "                    AUC = roc_auc_score(y_test_dl, preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    AUC_scores.append(AUC)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                    \n",
    "                # Calculate the average F1 score for the current configuration\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "             \n",
    "\n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "                print('AUC score: %.3f (+/- %.3f)' % (np.mean(AUC_scores), np.std(AUC_scores)))\n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat_optim(XGBClassifier())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing on the train - test set before prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temporal, X_train_static, y_train_preprocessed, X_test_temporal,\n",
    "X_test_static, y_test_preprocessed = preprocess_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_steps, num_features = X_train_temporal.shape[1], X_train_temporal.shape[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized temporal models (Dense layer)\n",
    "\n",
    "def RNN_Temporal():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32,return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(SimpleRNN(32,return_sequences = True))\n",
    "    model.add(SimpleRNN(32,return_sequences = True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def LSTM_Temporal():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def CNN_Temporal():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=4, kernel_size=5, padding='same', activation='relu',\n",
    "                     input_shape=(num_time_steps, num_features)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=8, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized temporal models (with ML in FCL)\n",
    "\n",
    "def RNN_Temporal_LR():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(SimpleRNN(32, return_sequences=True))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def LSTM_Temporal_LR():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(16, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def CNN_Temporal_LR():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=4, kernel_size=5, padding='same', activation='relu', \n",
    "                     input_shape=(num_time_steps, num_features)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=8, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def RNN_Temporal_RF():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def LSTM_Temporal_RF():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def CNN_Temporal_RF():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=8, kernel_size=5, padding='same', activation='relu',\n",
    "                     input_shape=(num_time_steps, num_features)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def RNN_Temporal_XGB():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(16, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def LSTM_Temporal_XGB():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(16, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def CNN_Temporal_XGB():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=16, kernel_size=5, padding='same', activation='relu', \n",
    "                     input_shape=(num_time_steps, num_features)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and evaluating on test set for optimized configurations (temporal - dense)\n",
    "\n",
    "def temporal_dense_evaluation(X_train, y_train, X_test, y_test, batch_size, optimizer, \n",
    "                              initial_learning_rate, model):\n",
    "    \n",
    "    decay_rate = 0.1  # Decay rate\n",
    "    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "    epochs = 50\n",
    "\n",
    "    def learning_rate_scheduler(epoch):\n",
    "        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "    \n",
    "    if optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test), shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "    # Plot the loss on train vs validate tests\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on the testing set\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    test_accuracy =  accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred)\n",
    "    test_recall = recall_score(y_test, y_pred)\n",
    "    test_f1 =  f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    y_train_pred_probs = model.predict(X_train)\n",
    "    y_train_pred = (y_train_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "   \n",
    "    print(f\"Testing Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % test_accuracy)\n",
    "    print(\"Precision: %.3f\" % test_precision)\n",
    "    print(\"Recall: %.3f\" % test_recall)\n",
    "    print(\"F1 score: %.3f\" % test_f1)\n",
    "  \n",
    "    print('-----------------------------------')\n",
    "    print(f\"Training Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Precision: %.3f\" % train_precision)\n",
    "    print(\"Recall: %.3f\" % train_recall)\n",
    "    print(\"F1 score: %.3f\" % train_f1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and evaluating on test set for optimized configurations (temporal - ML)\n",
    "\n",
    "def temporal_ML_evaluation(X_train, y_train, X_test, y_test, batch_size, optimizer, \n",
    "                           initial_learning_rate, model, ml_model):\n",
    "    \n",
    "    decay_rate = 0.1  # Decay rate\n",
    "    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "    epochs = 50\n",
    "\n",
    "    def learning_rate_scheduler(epoch):\n",
    "        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "    \n",
    "    if optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_data=(X_test, y_test), shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "    # Plot the loss on train vs validate tests\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    preds,preds_train = classifier_prediction_temporal(X_train,X_test, y_train, \n",
    "                                                       model = ml_model, feature_extractor_model= model, \n",
    "                                                       layer_name='FLATTEN')\n",
    "    \n",
    "    \n",
    "    # Evaluate on the testing set\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    test_precision = precision_score(y_test, preds)\n",
    "    test_recall = recall_score(y_test, preds)\n",
    "    test_f1 =  f1_score(y_test, preds)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    train_accuracy = accuracy_score(y_train, preds_train)\n",
    "    train_precision = precision_score(y_train, preds_train)\n",
    "    train_recall = recall_score(y_train, preds_train)\n",
    "    train_f1 = f1_score(y_train, preds_train)\n",
    "   \n",
    "    print(f\"Testing Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % test_accuracy)\n",
    "    print(\"Precision: %.3f\" % test_precision)\n",
    "    print(\"Recall: %.3f\" % test_recall)\n",
    "    print(\"F1 score: %.3f\" % test_f1)\n",
    "  \n",
    "    print('-----------------------------------')\n",
    "    print(f\"Training Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Precision: %.3f\" % train_precision)\n",
    "    print(\"Recall: %.3f\" % train_recall)\n",
    "    print(\"F1 score: %.3f\" % train_f1)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_dense_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=32,\n",
    "                          optimizer='rmsprop',initial_learning_rate=0.0001, model=RNN_Temporal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_dense_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                          optimizer='adam',initial_learning_rate=0.0001, model=LSTM_Temporal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_dense_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                          optimizer='adam',initial_learning_rate=0.001, model=CNN_Temporal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=128,\n",
    "                       optimizer='rmsprop',initial_learning_rate=0.0001, \n",
    "                       model=RNN_Temporal_LR(),ml_model = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='adam',initial_learning_rate=0.001, \n",
    "                       model=RNN_Temporal_RF(),ml_model = RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='adam',initial_learning_rate=0.0001, \n",
    "                       model=RNN_Temporal_XGB(),ml_model = XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=32,\n",
    "                       optimizer='adam',initial_learning_rate=0.0001, \n",
    "                       model=LSTM_Temporal_LR(),ml_model = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='rmsprop',initial_learning_rate=0.001,\n",
    "                       model=LSTM_Temporal_RF(),ml_model = RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=32,\n",
    "                       optimizer='rmsprop',initial_learning_rate=0.0001, \n",
    "                       model=LSTM_Temporal_XGB(),ml_model = XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='adam',initial_learning_rate=0.001, \n",
    "                       model=CNN_Temporal_LR(),ml_model = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='adam',initial_learning_rate=0.001, \n",
    "                       model=CNN_Temporal_RF(),ml_model = RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='adam',initial_learning_rate=0.001, \n",
    "                       model=CNN_Temporal_XGB(),ml_model = XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static + Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Concat models (Dense layer)\n",
    "\n",
    "def RNN_Concat():\n",
    "    rnn_layer = SimpleRNN(64,return_sequences=True, name = 'RNN_LAYER_1')(temporal_input)\n",
    "    rnn_layer = SimpleRNN(64,return_sequences=True, name = 'RNN_LAYER_2')(rnn_layer)\n",
    "    rnn_layer = SimpleRNN(64,return_sequences=True, name = 'RNN_LAYER_3')(rnn_layer)\n",
    "    rnn_layer = Flatten(name = 'FLATTEN')(rnn_layer)\n",
    "    RNN_combined = Concatenate(axis=1, name ='RNN_CONCAT')([rnn_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def LSTM_Concat():\n",
    "    lstm_layer = LSTM(64,return_sequences=True, name = 'LSTM_LAYER_1')(temporal_input)\n",
    "    lstm_layer = LSTM(64,return_sequences=True, name = 'LSTM_LAYER_2')(lstm_layer)\n",
    "    lstm_layer = LSTM(64,return_sequences=True, name = 'LSTM_LAYER_3')(lstm_layer)\n",
    "    lstm_layer = LSTM(64,return_sequences=True, name = 'LSTM_LAYER_4')(lstm_layer)\n",
    "    lstm_layer = Flatten(name = 'FLATTEN')(lstm_layer)\n",
    "    LSTM_combined = Concatenate(axis=1, name ='LSTM_CONCAT')([lstm_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def CNN_Concat():\n",
    "    cnn_layer = Conv1D(filters=256, kernel_size=5, activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "    cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "    cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "    CNN_combined = Concatenate(axis=1, name='cnn_CONCAT')([cnn_layer, static_input])\n",
    "    output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(CNN_combined)\n",
    "    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Concat models (with ML in FCL)\n",
    "\n",
    "def RNN_Concat_LR():\n",
    "    rnn_layer = SimpleRNN(32,return_sequences=True, name = 'RNN_LAYER_1')(temporal_input)\n",
    "    rnn_layer = SimpleRNN(32,return_sequences=True, name = 'RNN_LAYER_2')(rnn_layer)\n",
    "    rnn_layer = Flatten(name = 'FLATTEN')(rnn_layer)\n",
    "    RNN_combined = Concatenate(axis=1, name ='CONCAT')([rnn_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def LSTM_Concat_LR():\n",
    "    lstm_layer = LSTM(32,return_sequences=True, name = 'LSTM_LAYER_1')(temporal_input)\n",
    "    lstm_layer = LSTM(32,return_sequences=True, name = 'LSTM_LAYER_2')(lstm_layer)\n",
    "    lstm_layer = Flatten(name = 'FLATTEN')(lstm_layer)\n",
    "    LSTM_combined = Concatenate(axis=1, name ='CONCAT')([lstm_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def CNN_Concat_LR():\n",
    "    cnn_layer = Conv1D(filters=4, kernel_size=5, activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "    cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "    cnn_layer = Conv1D(filters=8, kernel_size=3, activation='relu',padding='same', name=f'CNN_LAYER_2')(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "    CNN_combined = Concatenate(axis=1, name='CONCAT')([cnn_layer, static_input])\n",
    "    output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(CNN_combined)\n",
    "    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "    return model\n",
    "\n",
    "def RNN_Concat_RF():\n",
    "    rnn_layer = SimpleRNN(16,return_sequences=True, name = 'RNN_LAYER_1')(temporal_input)\n",
    "    rnn_layer = Flatten(name = 'FLATTEN')(rnn_layer)\n",
    "    RNN_combined = Concatenate(axis=1, name ='CONCAT')([rnn_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def LSTM_Concat_RF():\n",
    "    lstm_layer = LSTM(16,return_sequences=True, name = 'LSTM_LAYER_1')(temporal_input)\n",
    "    lstm_layer = Flatten(name = 'FLATTEN')(lstm_layer)\n",
    "    LSTM_combined = Concatenate(axis=1, name ='CONCAT')([lstm_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def CNN_Concat_RF():\n",
    "    cnn_layer = Conv1D(filters=8, kernel_size=5, activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "    cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "    cnn_layer = Conv1D(filters=16, kernel_size=3, activation='relu',padding='same', name=f'CNN_LAYER_2')(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "    CNN_combined = Concatenate(axis=1, name='CONCAT')([cnn_layer, static_input])\n",
    "    output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(CNN_combined)\n",
    "    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "    return model\n",
    "\n",
    "def RNN_Concat_XGB():\n",
    "    rnn_layer = SimpleRNN(128,return_sequences=True, name = 'RNN_LAYER_1')(temporal_input)\n",
    "    rnn_layer = SimpleRNN(128,return_sequences=True, name = 'RNN_LAYER_2')(rnn_layer)\n",
    "    rnn_layer = Flatten(name = 'FLATTEN')(rnn_layer)\n",
    "    RNN_combined = Concatenate(axis=1, name ='CONCAT')([rnn_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def LSTM_Concat_XGB():\n",
    "    lstm_layer = LSTM(32,return_sequences=True, name = 'LSTM_LAYER_1')(temporal_input)\n",
    "    lstm_layer = LSTM(32,return_sequences=True, name = 'LSTM_LAYER_2')(lstm_layer)\n",
    "    lstm_layer = LSTM(32,return_sequences=True, name = 'LSTM_LAYER_3')(lstm_layer)\n",
    "    lstm_layer = Flatten(name = 'FLATTEN')(lstm_layer)\n",
    "    LSTM_combined = Concatenate(axis=1, name ='CONCAT')([lstm_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def CNN_Concat_XGB():\n",
    "    cnn_layer = Conv1D(filters=4, kernel_size=5, activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "    cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "    cnn_layer = Conv1D(filters=8, kernel_size=3, activation='relu',padding='same', name=f'CNN_LAYER_2')(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "    CNN_combined = Concatenate(axis=1, name='CONCAT')([cnn_layer, static_input])\n",
    "    output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(CNN_combined)\n",
    "    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "temporal_input = Input(shape=(num_time_steps, num_features),name = 'TEMPORAL_INPUT')\n",
    "static_input = Input(shape=(X_train_static.shape[1]),name = 'STATIC_INPUT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and evaluating on test set for optimized configurations (concat - dense)\n",
    "\n",
    "def concat_dense_evaluation(X_train_temporal,X_train_static, y_train, X_test_temporal,\n",
    "                            X_test_static, y_test, batch_size, optimizer, initial_learning_rate, model):\n",
    "    \n",
    "    decay_rate = 0.1  # Decay rate\n",
    "    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "    epochs = 50\n",
    "\n",
    "    def learning_rate_scheduler(epoch):\n",
    "        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "    \n",
    "    if optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([X_train_temporal, X_train_static], y_train, epochs=epochs, batch_size=batch_size,\n",
    "                        validation_data=([X_test_temporal, X_test_static], y_test),\n",
    "                        shuffle=False,callbacks = [lr_scheduler])\n",
    "\n",
    "    # Plot the loss on train vs validate tests\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on the testing set\n",
    "    y_pred_probs = model.predict([X_test_temporal, X_test_static])\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    test_accuracy =  accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred)\n",
    "    test_recall = recall_score(y_test, y_pred)\n",
    "    test_f1 =  f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    y_train_pred_probs = model.predict([X_train_temporal, X_train_static])\n",
    "    y_train_pred = (y_train_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "   \n",
    "    print(f\"Testing Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % test_accuracy)\n",
    "    print(\"Precision: %.3f\" % test_precision)\n",
    "    print(\"Recall: %.3f\" % test_recall)\n",
    "    print(\"F1 score: %.3f\" % test_f1)\n",
    "  \n",
    "    print('-----------------------------------')\n",
    "    print(f\"Training Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Precision: %.3f\" % train_precision)\n",
    "    print(\"Recall: %.3f\" % train_recall)\n",
    "    print(\"F1 score: %.3f\" % train_f1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and evaluating on test set for optimized configurations (concat - ML)\n",
    "\n",
    "def concat_ML_evaluation(X_train_temporal,X_train_static, y_train, X_test_temporal, \n",
    "                         X_test_static, y_test, batch_size, optimizer, initial_learning_rate, model, ml_model):\n",
    "    \n",
    "    decay_rate = 0.1  # Decay rate\n",
    "    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "    epochs = 50\n",
    "\n",
    "    def learning_rate_scheduler(epoch):\n",
    "        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "    \n",
    "    if optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([X_train_temporal, X_train_static], y_train,\n",
    "                                                            epochs=epochs, batch_size=batch_size,\n",
    "                                                            validation_data=([X_test_temporal, X_test_static], y_test),\n",
    "                                                            shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "    # Plot the loss on train vs validate tests\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    preds,preds_train = classifier_prediction(X_train_temporal,X_test_temporal,X_train_static, \n",
    "                                              X_test_static,y_train,y_test, model = ml_model, \n",
    "                                              feature_extractor_model = model, layer_name='CONCAT')\n",
    "    \n",
    "    \n",
    "    # Evaluate on the testing set\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    test_precision = precision_score(y_test, preds)\n",
    "    test_recall = recall_score(y_test, preds)\n",
    "    test_f1 =  f1_score(y_test, preds)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    train_accuracy = accuracy_score(y_train, preds_train)\n",
    "    train_precision = precision_score(y_train, preds_train)\n",
    "    train_recall = recall_score(y_train, preds_train)\n",
    "    train_f1 = f1_score(y_train, preds_train)\n",
    "   \n",
    "    print(f\"Testing Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % test_accuracy)\n",
    "    print(\"Precision: %.3f\" % test_precision)\n",
    "    print(\"Recall: %.3f\" % test_recall)\n",
    "    print(\"F1 score: %.3f\" % test_f1)\n",
    "  \n",
    "    print('-----------------------------------')\n",
    "    print(f\"Training Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Precision: %.3f\" % train_precision)\n",
    "    print(\"Recall: %.3f\" % train_recall)\n",
    "    print(\"F1 score: %.3f\" % train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dense_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static, \n",
    "                        y_test_preprocessed, batch_size=64, optimizer='adam',\n",
    "                        initial_learning_rate=0.0001, model=RNN_Concat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dense_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                        y_test_preprocessed, batch_size=64, optimizer='adam', \n",
    "                        initial_learning_rate=0.0001, model=LSTM_Concat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dense_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                        y_test_preprocessed, batch_size=64, optimizer='adam', \n",
    "                        initial_learning_rate=0.001, model=CNN_Concat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                     y_test_preprocessed, batch_size=128, optimizer='adam',\n",
    "                     initial_learning_rate=0.0001, model=RNN_Concat_LR(), ml_model=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal,\n",
    "                     X_test_static, y_test_preprocessed, batch_size=32, optimizer='rmsprop', \n",
    "                     initial_learning_rate=0.0001, model=RNN_Concat_RF(), ml_model=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal,\n",
    "                     X_test_static, y_test_preprocessed, batch_size=128, optimizer='adam',\n",
    "                     initial_learning_rate=0.0001, model=RNN_Concat_XGB(), ml_model=XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                     y_test_preprocessed, batch_size=64, optimizer='adam', \n",
    "                     initial_learning_rate=0.0001, model=LSTM_Concat_LR(), ml_model=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal,\n",
    "                     X_test_static, y_test_preprocessed, batch_size=64, optimizer='adam',\n",
    "                     initial_learning_rate=0.001, model=LSTM_Concat_RF(), ml_model=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal,\n",
    "                     X_test_static, y_test_preprocessed, batch_size=64, optimizer='adam',\n",
    "                     initial_learning_rate=0.0001, model=LSTM_Concat_XGB(), ml_model=XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                     y_test_preprocessed, batch_size=32, optimizer='adam', \n",
    "                     initial_learning_rate=0.001, model=CNN_Concat_LR(), ml_model=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                     y_test_preprocessed, batch_size=64, optimizer='adam', \n",
    "                     initial_learning_rate=0.001, model=CNN_Concat_RF(), ml_model=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                     y_test_preprocessed, batch_size=64, optimizer='adam', \n",
    "                     initial_learning_rate=0.001, model=CNN_Concat_XGB(), ml_model=XGBClassifier())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
