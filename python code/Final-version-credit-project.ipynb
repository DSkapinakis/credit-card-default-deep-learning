{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### whole code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries - Setting seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Manipulation libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing - Cross valdidation - Evaluation metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow/Keras library - Deep Learning models\n",
    "import tensorflow\n",
    "from keras.layers import SimpleRNN, LSTM, Conv1D, MaxPooling1D, Flatten, Concatenate, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import Input, Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting seed libraries\n",
    "import os\n",
    "import random\n",
    "from tensorflow.random import set_seed\n",
    "from keras.utils import set_random_seed\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed value to 42\n",
    "seed_value= 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tensorflow.random.set_seed(seed_value)\n",
    "session_conf = tensorflow.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tensorflow.compat.v1.Session(graph=tensorflow.compat.v1.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing - Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...          0          0          0         0       689         0   \n",
       "1  ...       3272       3455       3261         0      1000      1000   \n",
       "2  ...      14331      14948      15549      1518      1500      1000   \n",
       "3  ...      28314      28959      29547      2000      2019      1200   \n",
       "4  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0         0         0         0                           1  \n",
       "1      1000         0      2000                           1  \n",
       "2      1000      1000      5000                           0  \n",
       "3      1100      1069      1000                           0  \n",
       "4      9000       689       679                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing data\n",
    "data = pd.read_excel('default of credit card clients.xls',header=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for initial preprocessing of the data:\n",
    "1) replace EDUCATION values 0, 5, 6 with 4 ('other' category) since they are not mentioned in the data description\n",
    "2) replace MARRIAGE value 0 with 3 ('other' category) as there is not a 0 category for marriage column on data description \n",
    "3) drop 'ID' column - useless\n",
    "4) rename target column to DEFAULT, rename PAY_0 to PAY_1 for consistency and more accurate variable names\n",
    "'''\n",
    "def initial_preprocessing(df):\n",
    "    print('EDUCATION values before preprocessing:\\n',df['EDUCATION'].value_counts())\n",
    "    df['EDUCATION'].replace([0,5,6],4,inplace=True)\n",
    "    print()\n",
    "    print('EDUCATION values after preprocessing:\\n',df['EDUCATION'].value_counts())\n",
    "    print()\n",
    "    print('MARRIAGE values before preprocessing:\\n',df['MARRIAGE'].value_counts())\n",
    "    df['MARRIAGE'].replace(0,3,inplace=True)\n",
    "    print()\n",
    "    print('MARRIAGE values after preprocessing:\\n',df['MARRIAGE'].value_counts())\n",
    "    df.drop(columns='ID',inplace=True)\n",
    "    df.rename(columns={\"default payment next month\": \"DEFAULT\",\"PAY_0\": \"PAY_1\"},inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDUCATION values before preprocessing:\n",
      " 2    14030\n",
      "1    10585\n",
      "3     4917\n",
      "5      280\n",
      "4      123\n",
      "6       51\n",
      "0       14\n",
      "Name: EDUCATION, dtype: int64\n",
      "\n",
      "EDUCATION values after preprocessing:\n",
      " 2    14030\n",
      "1    10585\n",
      "3     4917\n",
      "4      468\n",
      "Name: EDUCATION, dtype: int64\n",
      "\n",
      "MARRIAGE values before preprocessing:\n",
      " 2    15964\n",
      "1    13659\n",
      "3      323\n",
      "0       54\n",
      "Name: MARRIAGE, dtype: int64\n",
      "\n",
      "MARRIAGE values after preprocessing:\n",
      " 2    15964\n",
      "1    13659\n",
      "3      377\n",
      "Name: MARRIAGE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#perform initial preprocessing\n",
    "data = initial_preprocessing(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (24000, 23)\n",
      "X_test shape: (6000, 23)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- train test split (20% test) before scaling and encoding to prevent data leakages \n",
    "- train set will be used for cross-validation/hyperparameter tuning and test set for final evaluation \n",
    "- Stratify is used to ensure that the proportion of the class labels will remain consistent\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('DEFAULT',axis=1),data['DEFAULT'],\n",
    "                                                    test_size=0.2,stratify=data['DEFAULT'],random_state=42)\n",
    "print('X_train shape:',X_train.shape)\n",
    "print('X_test shape:',X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column transformer for Robust Scaler and One Hot Encoder \n",
    "## making sure to return the preprocessed dataframes for better inspection\n",
    "\n",
    "class PreprocessorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns,columns_num, drop='first', handle_unknown='ignore',sparse_output=False):\n",
    "        self.columns = columns\n",
    "        self.columns_num = columns_num\n",
    "        self.drop = drop\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.sparse_output = sparse_output\n",
    "        self.encoders = {}\n",
    "        self.robust_enc = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            encoder = OneHotEncoder(drop=self.drop, sparse_output=self.sparse_output, \n",
    "                                    handle_unknown=self.handle_unknown)\n",
    "            encoder.fit(X[[col]])\n",
    "            self.encoders[col] = encoder\n",
    "        \n",
    "        for col_num in self.columns_num:\n",
    "            encoder_robust = RobustScaler()\n",
    "            encoder_robust.fit(X[[col_num]])\n",
    "            self.robust_enc[col_num] = encoder_robust\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed = X.copy()\n",
    "        for col in self.columns:\n",
    "            encoder = self.encoders[col]\n",
    "            encoded_cols = encoder.transform(transformed[[col]])\n",
    "            new_cols = [f\"{col}_{value}\" for value in encoder.categories_[0][1:]]\n",
    "            encoded_cols_df = pd.DataFrame(encoded_cols, columns=new_cols, index=transformed.index)\n",
    "            transformed = pd.concat([transformed, encoded_cols_df], axis=1)\n",
    "        transformed = transformed.drop(self.columns, axis=1)\n",
    "        \n",
    "        for col_num in self.columns_num:\n",
    "            encoder_robust = self.robust_enc[col_num]\n",
    "            transformed[col_num] = encoder_robust.transform(transformed[[col_num]])\n",
    "            \n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    cat_cols = ['EDUCATION','MARRIAGE']\n",
    "    numerical_cols = ['LIMIT_BAL', 'AGE','BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5',\n",
    "                  'BILL_AMT6','PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "    \n",
    "    temp_cols = ['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5',\n",
    "    'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n",
    "    'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n",
    "    'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "    \n",
    "    PAY_cols = ['PAY_6', 'PAY_5', 'PAY_4', 'PAY_3', 'PAY_2','PAY_1']\n",
    "    BILL_AMT_cols = ['BILL_AMT6','BILL_AMT5','BILL_AMT4','BILL_AMT3','BILL_AMT2','BILL_AMT1']\n",
    "    PAY_AMT_cols = ['PAY_AMT6','PAY_AMT5', 'PAY_AMT4', 'PAY_AMT3', 'PAY_AMT2', 'PAY_AMT1']\n",
    "    \n",
    "    #fitting the column transformer\n",
    "    enc = PreprocessorTransformer(columns = cat_cols, columns_num= numerical_cols,\n",
    "                                  drop='first',handle_unknown='ignore',sparse_output=False) \n",
    "    \n",
    "    X_train_preprocessed = enc.fit_transform(X_train)\n",
    "    X_test_preprocessed = enc.transform(X_test)\n",
    "    \n",
    "    static_cols_train = X_train_preprocessed.drop(temp_cols,axis=1).columns.to_list()\n",
    "    static_cols_test = X_test_preprocessed.drop(temp_cols,axis=1).columns.to_list()\n",
    "    \n",
    "    #separation of static and temporal features\n",
    "    X_train_temp = X_train_preprocessed[temp_cols]\n",
    "    X_train_static = X_train_preprocessed[static_cols_train]\n",
    "    X_test_temp = X_test_preprocessed[temp_cols]\n",
    "    X_test_static = X_test_preprocessed[static_cols_test]\n",
    "\n",
    "    PAY_train = X_train_temp[PAY_cols].to_numpy()\n",
    "    BILL_AMT_train = X_train_temp[BILL_AMT_cols].to_numpy()\n",
    "    PAY_AMT_train = X_train_temp[PAY_AMT_cols].to_numpy()\n",
    "    \n",
    "    PAY_test = X_test_temp[PAY_cols].to_numpy()\n",
    "    BILL_AMT_test = X_test_temp[BILL_AMT_cols].to_numpy()\n",
    "    PAY_AMT_test = X_test_temp[PAY_AMT_cols].to_numpy()   \n",
    "    \n",
    "    # Stacking temporal features\n",
    "    stacked_train = np.dstack((PAY_train, BILL_AMT_train, PAY_AMT_train))\n",
    "    stacked_test = np.dstack((PAY_test, BILL_AMT_test, PAY_AMT_test))\n",
    "    y_train_preprocessed = y_train.to_numpy()\n",
    "    y_test_preprocessed = y_test.to_numpy()\n",
    "    \n",
    "    return stacked_train, X_train_static, y_train_preprocessed, stacked_test, X_test_static, y_test_preprocessed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "n_splits = 5 \n",
    "kf = StratifiedKFold(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to receive temporal features and make predictions with the ML model as FCL\n",
    "def classifier_prediction_temporal(X_train_temp, X_test_temp, y_train_prep,\n",
    "                                   model, feature_extractor_model, layer_name):\n",
    "    \n",
    "    # Extract features using the feature_extractor_model\n",
    "    extractor_model = Model(inputs=feature_extractor_model.input, \n",
    "                            outputs=feature_extractor_model.get_layer(name=layer_name).output)\n",
    "    customers_vector = extractor_model.predict(X_train_temp)\n",
    "    customers_test = extractor_model.predict(X_test_temp)\n",
    "    \n",
    "    # Reshape the extracted features\n",
    "    reshaped_customers_vector = customers_vector.reshape(customers_vector.shape[0], -1)\n",
    "    reshaped_customers_test = customers_test.reshape(customers_test.shape[0], -1) \n",
    "    \n",
    "    # Train the classification model with the extracted features\n",
    "    final_model = model\n",
    "    final_model.fit(reshaped_customers_vector, y_train_prep)\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    preds = final_model.predict(reshaped_customers_test)\n",
    "    preds_train = final_model.predict(reshaped_customers_vector)\n",
    "    \n",
    "    return preds, preds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to receive both static and temporal features and make predictions with the ML model as FCL\n",
    "def classifier_prediction(X_train_temp,X_test_temp,X_train_st,\n",
    "                          X_test_st,y_train_prep,y_test_prep,model, feature_extractor_model, layer_name):\n",
    "    \n",
    "    extractor_model = Model(inputs=feature_extractor_model.input, \n",
    "                            outputs=feature_extractor_model.get_layer(name=layer_name).output)\n",
    "    customers_vector = extractor_model.predict([X_train_temp,X_train_st])\n",
    "    customers_test = extractor_model.predict([X_test_temp,X_test_st])\n",
    "    reshaped_customers_vector = customers_vector.reshape(customers_vector.shape[0], -1)\n",
    "    reshaped_customers_test = customers_test.reshape(customers_test.shape[0], -1) \n",
    "    final_model = model\n",
    "    final_model.fit(reshaped_customers_vector, y_train_prep)\n",
    "    preds = final_model.predict(reshaped_customers_test)\n",
    "    preds_train = final_model.predict(reshaped_customers_vector)\n",
    "    \n",
    "    return preds, preds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 4s 8ms/step - loss: 0.5299 - accuracy: 0.7480 - val_loss: 0.4778 - val_accuracy: 0.7890 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4663 - accuracy: 0.7997 - val_loss: 0.4663 - val_accuracy: 0.7983 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "274/300 [==========================>...] - ETA: 0s - loss: 0.4577 - accuracy: 0.8044"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     55\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(stacked_train, y_train_dl, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     57\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(stacked_test, y_test_dl),\n\u001b[0;32m     58\u001b[0m                     shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,callbacks\u001b[39m=\u001b[39;49m[lr_scheduler])\n\u001b[0;32m     60\u001b[0m \u001b[39m# Plot the loss on train vs validate tests\u001b[39;00m\n\u001b[0;32m     61\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain Loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "architectures = [\n",
    "    {'hidden_layers': 4, 'units_per_layer': 32},\n",
    "    {'hidden_layers': 3, 'units_per_layer': 128}    \n",
    "]\n",
    "\n",
    "best_architecture = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for architecture in architectures:\n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test,\\\n",
    "            X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "        \n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "        \n",
    "        # Build the RNN model\n",
    "        model = Sequential()\n",
    "        model.add(SimpleRNN(architecture['units_per_layer'], return_sequences=True, \n",
    "                            input_shape=(num_time_steps, num_features)))\n",
    "        for _ in range(1, architecture['hidden_layers']):\n",
    "            model.add(SimpleRNN(architecture['units_per_layer'], return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        initial_learning_rate = 0.0001  # Initial learning rate\n",
    "        decay_rate = 0.1  # Decay rate\n",
    "        decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "        \n",
    "        \n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                      optimizer=Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])\n",
    "        \n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "    \n",
    "        batch_size = 64\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                            validation_data=(stacked_test, y_test_dl),\n",
    "                            shuffle=False,callbacks=[lr_scheduler])\n",
    "        \n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        y_pred_probs = model.predict(stacked_test)\n",
    "        y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "        \n",
    "        accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "        precision = precision_score(y_test_dl,y_pred)\n",
    "        recall = recall_score(y_test_dl,y_pred)\n",
    "        f1 =  f1_score(y_test_dl,y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "        \n",
    "    # Calculate the average F1 score for the current architecture\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    # Print the scores for the current architecture\n",
    "    print(f\"Architecture: {architecture}\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print()\n",
    "    \n",
    "    # Check if the current architecture has a higher average F1 score\n",
    "    # If so, update the best architecture and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_architecture = architecture\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best architecture\n",
    "print(\"Best Architecture:\")\n",
    "print(best_architecture)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     55\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(stacked_train, y_train_dl, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[0;32m     57\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(stacked_test, y_test_dl),\n\u001b[0;32m     58\u001b[0m                     shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,callbacks\u001b[39m=\u001b[39;49m[lr_scheduler])\n\u001b[0;32m     60\u001b[0m \u001b[39m# Plot the loss on train vs validate tests\u001b[39;00m\n\u001b[0;32m     61\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain Loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[39m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    983\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2495\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2492\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m-> 2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mgraph_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2760\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2758\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[0;32m   2759\u001b[0m   args, kwargs \u001b[39m=\u001b[39m placeholder_dict[\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m-> 2760\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   2762\u001b[0m graph_capture_container \u001b[39m=\u001b[39m graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2670\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   2666\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2667\u001b[0m ]\n\u001b[0;32m   2668\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   2669\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 2670\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   2671\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   2672\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   2673\u001b[0m         args,\n\u001b[0;32m   2674\u001b[0m         kwargs,\n\u001b[0;32m   2675\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   2676\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   2677\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   2678\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   2679\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   2680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   2681\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m   2682\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   2683\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   2684\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   2686\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   2687\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1247\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1247\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1249\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m   1252\u001b[0m     convert, func_outputs, expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:677\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    674\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    675\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    676\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 677\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39m__wrapped__(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    678\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1222\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[39m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1222\u001b[0m   \u001b[39mreturn\u001b[39;00m autograph\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[0;32m   1223\u001b[0m       original_func,\n\u001b[0;32m   1224\u001b[0m       args,\n\u001b[0;32m   1225\u001b[0m       kwargs,\n\u001b[0;32m   1226\u001b[0m       options\u001b[39m=\u001b[39;49mautograph\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[0;32m   1227\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1228\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[0;32m   1229\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1230\u001b[0m       ))\n\u001b[0;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7hre92x5.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(step_function), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), ag__\u001b[39m.\u001b[39;49mld(iterator)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m conversion\u001b[39m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[0;32m    330\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: from cache\u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[1;32m--> 331\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mcontrol_status_ctx()\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m ag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mDISABLED:\n\u001b[0;32m    334\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: AutoGraph is disabled in context\u001b[39m\u001b[39m'\u001b[39m, f)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\training.py:1146\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1143\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[0;32m   1145\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1146\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1147\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1148\u001b[0m     outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1149\u001b[0m )\n\u001b[0;32m   1150\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1315\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[0;32m   1311\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   1314\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 1315\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2891\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2889\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m   2890\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 2891\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3692\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   3691\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m-> 3692\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\training.py:1135\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1135\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1136\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\training.py:993\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[39m# Run forward pass.\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m--> 993\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    994\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[0;32m    995\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\training.py:557\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    555\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\sequential.py:410\u001b[0m, in \u001b[0;36mSequential.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    408\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m    409\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_graph_network(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)\n\u001b[1;32m--> 410\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcall(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n\u001b[0;32m    412\u001b[0m outputs \u001b[39m=\u001b[39m inputs  \u001b[39m# handle the corner case where self.layers is empty\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m    414\u001b[0m     \u001b[39m# During each iteration, `inputs` are the inputs to `layer`, and\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     \u001b[39m# `outputs` are the outputs of `layer` applied to `inputs`. At the\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     \u001b[39m# end of each iteration `inputs` is set to `outputs` to prepare for\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[39m# the next layer.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\functional.py:510\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    493\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \n\u001b[0;32m    495\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\functional.py:667\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 667\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    669\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    671\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[0;32m    672\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py:553\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39mstandardize_args(\n\u001b[0;32m    549\u001b[0m     inputs, initial_state, constants, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants\n\u001b[0;32m    550\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    555\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py:607\u001b[0m, in \u001b[0;36mLSTM.call\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args_if_ragged(is_ragged_input, mask)\n\u001b[0;32m    606\u001b[0m \u001b[39m# LSTM does not support constants. Ignore it during process.\u001b[39;00m\n\u001b[1;32m--> 607\u001b[0m inputs, initial_state, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_inputs(\n\u001b[0;32m    608\u001b[0m     inputs, initial_state, \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    609\u001b[0m )\n\u001b[0;32m    611\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    612\u001b[0m     mask \u001b[39m=\u001b[39m mask[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py:810\u001b[0m, in \u001b[0;36mRNN._process_inputs\u001b[1;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[0;32m    801\u001b[0m     initial_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m    802\u001b[0m         \u001b[39m# When the layer has a inferred dtype, use the dtype from the\u001b[39;00m\n\u001b[0;32m    803\u001b[0m         \u001b[39m# cell.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m         initial_state,\n\u001b[0;32m    808\u001b[0m     )\n\u001b[0;32m    809\u001b[0m \u001b[39melif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m     initial_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_initial_state(inputs)\n\u001b[0;32m    812\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(initial_state) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates):\n\u001b[0;32m    813\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    814\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLayer has \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstates but was passed \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(initial_state)\u001b[39m}\u001b[39;00m\u001b[39m initial \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstates. Received: initial_state=\u001b[39m\u001b[39m{\u001b[39;00minitial_state\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py:532\u001b[0m, in \u001b[0;36mRNN.get_initial_state\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    530\u001b[0m dtype \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    531\u001b[0m \u001b[39mif\u001b[39;00m get_initial_state_fn:\n\u001b[1;32m--> 532\u001b[0m     init_state \u001b[39m=\u001b[39m get_initial_state_fn(\n\u001b[0;32m    533\u001b[0m         inputs\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49mbatch_size, dtype\u001b[39m=\u001b[39;49mdtype\n\u001b[0;32m    534\u001b[0m     )\n\u001b[0;32m    535\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    536\u001b[0m     init_state \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39mgenerate_zero_filled_state(\n\u001b[0;32m    537\u001b[0m         batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell\u001b[39m.\u001b[39mstate_size, dtype\n\u001b[0;32m    538\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py:376\u001b[0m, in \u001b[0;36mLSTMCell.get_initial_state\u001b[1;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_initial_state\u001b[39m(\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    375\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[1;32m--> 376\u001b[0m         rnn_utils\u001b[39m.\u001b[39;49mgenerate_zero_filled_state_for_cell(\n\u001b[0;32m    377\u001b[0m             \u001b[39mself\u001b[39;49m, inputs, batch_size, dtype\n\u001b[0;32m    378\u001b[0m         )\n\u001b[0;32m    379\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\layers\\rnn\\rnn_utils.py:100\u001b[0m, in \u001b[0;36mgenerate_zero_filled_state_for_cell\u001b[1;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[0;32m     98\u001b[0m     batch_size \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mshape(inputs)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     99\u001b[0m     dtype \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mdtype\n\u001b[1;32m--> 100\u001b[0m \u001b[39mreturn\u001b[39;00m generate_zero_filled_state(batch_size, cell\u001b[39m.\u001b[39;49mstate_size, dtype)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\layers\\rnn\\rnn_utils.py:117\u001b[0m, in \u001b[0;36mgenerate_zero_filled_state\u001b[1;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mzeros(init_state_size, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mis_nested(state_size):\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(create_zeros, state_size)\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m create_zeros(state_size)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\keras\\layers\\rnn\\rnn_utils.py:114\u001b[0m, in \u001b[0;36mgenerate_zero_filled_state.<locals>.create_zeros\u001b[1;34m(unnested_state_size)\u001b[0m\n\u001b[0;32m    112\u001b[0m flat_dims \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mTensorShape(unnested_state_size)\u001b[39m.\u001b[39mas_list()\n\u001b[0;32m    113\u001b[0m init_state_size \u001b[39m=\u001b[39m [batch_size_tensor] \u001b[39m+\u001b[39m flat_dims\n\u001b[1;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mzeros(init_state_size, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2971\u001b[0m, in \u001b[0;36m_tag_zeros_tensor.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2970\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2971\u001b[0m   tensor \u001b[39m=\u001b[39m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2972\u001b[0m   tensor\u001b[39m.\u001b[39m_is_zeros_tensor \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2973\u001b[0m   \u001b[39mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:3032\u001b[0m, in \u001b[0;36mzeros\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   3030\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m shape\u001b[39m.\u001b[39m_shape_tuple():\n\u001b[0;32m   3031\u001b[0m     shape \u001b[39m=\u001b[39m reshape(shape, [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])  \u001b[39m# Ensure it's a vector\u001b[39;00m\n\u001b[1;32m-> 3032\u001b[0m   output \u001b[39m=\u001b[39m fill(shape, constant(zero, dtype\u001b[39m=\u001b[39;49mdtype), name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   3033\u001b[0m \u001b[39massert\u001b[39;00m output\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype \u001b[39m==\u001b[39m dtype\n\u001b[0;32m   3034\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:243\u001b[0m, in \u001b[0;36mfill\u001b[1;34m(dims, value, name)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfill\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    205\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfill\u001b[39m(dims, value, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m  \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Creates a tensor filled with a scalar value.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \n\u001b[0;32m    209\u001b[0m \u001b[39m  See also `tf.ones`, `tf.zeros`, `tf.one_hot`, `tf.eye`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39m  @end_compatibility\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m   result \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49mfill(dims, value, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    244\u001b[0m   tensor_util\u001b[39m.\u001b[39mmaybe_set_static_shape(result, dims)\n\u001b[0;32m    245\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:3513\u001b[0m, in \u001b[0;36mfill\u001b[1;34m(dims, value, name)\u001b[0m\n\u001b[0;32m   3511\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m   3512\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m-> 3513\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[0;32m   3514\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mFill\u001b[39;49m\u001b[39m\"\u001b[39;49m, dims\u001b[39m=\u001b[39;49mdims, value\u001b[39m=\u001b[39;49mvalue, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   3515\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[0;32m   3516\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:797\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    792\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    793\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[0;32m    794\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    795\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    796\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    798\u001b[0m                              name\u001b[39m=\u001b[39;49mscope, input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m    799\u001b[0m                              attrs\u001b[39m=\u001b[39;49mattr_protos, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m    801\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:735\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    733\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[0;32m    734\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m--> 735\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    736\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    737\u001b[0m     compute_device)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3800\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3797\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3800\u001b[0m   ret \u001b[39m=\u001b[39m Operation(\n\u001b[0;32m   3801\u001b[0m       node_def,\n\u001b[0;32m   3802\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   3803\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m   3804\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   3805\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[0;32m   3806\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   3807\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[0;32m   3808\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m   3809\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m   3810\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2108\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   2105\u001b[0m     control_input_ops\u001b[39m.\u001b[39mappend(control_op)\n\u001b[0;32m   2107\u001b[0m \u001b[39m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 2108\u001b[0m c_op \u001b[39m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m   2109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_from_c_op(c_op\u001b[39m=\u001b[39mc_op, g\u001b[39m=\u001b[39mg)\n\u001b[0;32m   2111\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_op \u001b[39m=\u001b[39m original_op\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diama\\anaconda3\\envs\\spyder\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1939\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1937\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1938\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39m_c_graph\u001b[39m.\u001b[39mget() \u001b[39mas\u001b[39;00m c_graph:\n\u001b[1;32m-> 1939\u001b[0m   op_desc \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_NewOperation(c_graph,\n\u001b[0;32m   1940\u001b[0m                                               compat\u001b[39m.\u001b[39;49mas_str(node_def\u001b[39m.\u001b[39;49mop),\n\u001b[0;32m   1941\u001b[0m                                               compat\u001b[39m.\u001b[39;49mas_str(node_def\u001b[39m.\u001b[39;49mname))\n\u001b[0;32m   1942\u001b[0m \u001b[39mif\u001b[39;00m node_def\u001b[39m.\u001b[39mdevice:\n\u001b[0;32m   1943\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[39m.\u001b[39mas_str(node_def\u001b[39m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "architectures = [\n",
    "    {'hidden_layers': 5, 'units_per_layer': 32},\n",
    "    {'hidden_layers': 6, 'units_per_layer': 32}\n",
    "]\n",
    "\n",
    "best_architecture = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for architecture in architectures:\n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static,\n",
    "        y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "        \n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "        \n",
    "        # Build the LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(architecture['units_per_layer'], return_sequences=True, \n",
    "                       input_shape=(num_time_steps, num_features)))\n",
    "        for _ in range(1, architecture['hidden_layers']):\n",
    "            model.add(LSTM(architecture['units_per_layer'], return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        initial_learning_rate = 0.0001  # Initial learning rate\n",
    "        decay_rate = 0.1  # Decay rate\n",
    "        decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "        \n",
    "        \n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate),\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "    \n",
    "        batch_size = 64\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size, \n",
    "                            validation_data=(stacked_test, y_test_dl),\n",
    "                            shuffle=False,callbacks=[lr_scheduler])\n",
    "        \n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        y_pred_probs = model.predict(stacked_test)\n",
    "        y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "        \n",
    "        accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "        precision = precision_score(y_test_dl,y_pred)\n",
    "        recall = recall_score(y_test_dl,y_pred)\n",
    "        f1 =  f1_score(y_test_dl,y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "        \n",
    "    # Calculate the average F1 score for the current architecture\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "    total_params = model.count_params()  \n",
    "\n",
    "    # Print the scores for the current architecture\n",
    "    print(f\"Architecture: {architecture}\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print('total params:', total_params)\n",
    "    print()\n",
    "    \n",
    "    # Check if the current architecture has a higher average F1 score\n",
    "    # If so, update the best architecture and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_architecture = architecture\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best architecture\n",
    "print(\"Best Architecture:\")\n",
    "print(best_architecture)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "\n",
    "first_layer_filters = [128,256]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for filter in first_layer_filters:\n",
    "    \n",
    "            \n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "        X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "        \n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "        \n",
    "        # Build the CNN model\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=filter, kernel_size=5, activation='relu',padding='same',\n",
    "                         input_shape=(num_time_steps, num_features)))\n",
    "        model.add(MaxPooling1D(pool_size=3))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        initial_learning_rate = 0.001  # Initial learning rate\n",
    "        decay_rate = 0.1  # Decay rate\n",
    "        decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                      optimizer=Adam(learning_rate = initial_learning_rate), metrics=['accuracy'])\n",
    "        \n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "        \n",
    "        batch_size = 64\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                            validation_data=(stacked_test, y_test_dl),\n",
    "                            shuffle=False,callbacks=[lr_scheduler])\n",
    "        \n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        y_pred_probs = model.predict(stacked_test)\n",
    "        y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "        \n",
    "        accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "        precision = precision_score(y_test_dl,y_pred)\n",
    "        recall = recall_score(y_test_dl,y_pred)\n",
    "        f1 =  f1_score(y_test_dl,y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "    \n",
    "    # Calculate the average F1 score for the current configuration\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "    total_params = model.count_params()\n",
    "\n",
    "    # Print the scores for the current configuration\n",
    "    print(f\"Configuration: filters=({filter})\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print(\"Total Trainable Parameters of CNN:\", total_params)\n",
    "    print()\n",
    "\n",
    "    # Check if the current configuration has a higher average F1 score\n",
    "    # If so, update the best configuration and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_configuration = (filter)\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration of filters:\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_rnn_temporal(class_model):\n",
    "   \n",
    "   \n",
    "#insert the respective architecture configurations to be tested   \n",
    "    architectures = [\n",
    "        {'hidden_layers': 4, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 4, 'units_per_layer': 16}\n",
    "        \n",
    "        ]\n",
    "\n",
    "    best_architecture = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for architecture in architectures:\n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, \n",
    "            stacked_test, X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, \n",
    "                                                                     X_test_dl, y_test_dl)\n",
    "        \n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "            \n",
    "            # Build the RNN model\n",
    "            model = Sequential()\n",
    "            model.add(SimpleRNN(architecture['units_per_layer'],\n",
    "                                return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "            for _ in range(1, architecture['hidden_layers']):\n",
    "                model.add(SimpleRNN(architecture['units_per_layer'], return_sequences=True))\n",
    "            model.add(Flatten(name='RNN_FLATTEN'))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            \n",
    "            initial_learning_rate = 0.0001  # Initial learning rate\n",
    "            decay_rate = 0.1  # Decay rate\n",
    "            decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "            \n",
    "            \n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate), \n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "        \n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "            # Train the model\n",
    "            history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(stacked_test, y_test_dl),\n",
    "                                shuffle=False,callbacks=[lr_scheduler])\n",
    "            \n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl,\n",
    "                                                   model = class_model, feature_extractor_model=model, \n",
    "                                                   layer_name='RNN_FLATTEN')[0]\n",
    "            \n",
    "            accuracy =  accuracy_score(y_test_dl,preds)\n",
    "            precision = precision_score(y_test_dl,preds)\n",
    "            recall = recall_score(y_test_dl,preds)\n",
    "            f1 =  f1_score(y_test_dl,preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "          \n",
    "        # Calculate the number of parameters for the current architecture\n",
    "        total_params = model.count_params()  \n",
    "            \n",
    "        # Calculate the average F1 score for the current architecture\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "        # Print the scores for the current architecture\n",
    "        print(f\"Architecture: {architecture}\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print(\"Total Trainable Parameters of RNN:\", total_params)\n",
    "        print()\n",
    "        \n",
    "        # Check if the current architecture has a higher average F1 score\n",
    "        # If so, update the best architecture and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_architecture = architecture\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best architecture\n",
    "    print(\"Best Architecture:\")\n",
    "    print(best_architecture)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_lstm_temporal(class_model):\n",
    "   \n",
    "   #insert the respective architecture configurations to be tested\n",
    "   \n",
    "    architectures = [\n",
    "        {'hidden_layers': 1, 'units_per_layer': 128},\n",
    "        {'hidden_layers': 2, 'units_per_layer': 64},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 64},\n",
    "        {'hidden_layers': 2, 'units_per_layer': 128},\n",
    "        {'hidden_layers': 2, 'units_per_layer': 256},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 128},\n",
    "        {'hidden_layers': 4, 'units_per_layer': 16},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 4, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 4, 'units_per_layer': 64},\n",
    "        {'hidden_layers': 5, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 6, 'units_per_layer': 32}\n",
    "    ]\n",
    "\n",
    "    best_architecture = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for architecture in architectures:\n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "            X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "            \n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "            \n",
    "            # Build the LSTM model\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(architecture['units_per_layer'],\n",
    "                           return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "            for _ in range(1, architecture['hidden_layers']):\n",
    "                model.add(LSTM(architecture['units_per_layer'], return_sequences=True))\n",
    "            model.add(Flatten(name='LSTM_FLATTEN'))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            \n",
    "            initial_learning_rate = 0.0001  # Initial learning rate\n",
    "            decay_rate = 0.1  # Decay rate\n",
    "            decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "            \n",
    "            \n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate), \n",
    "                          metrics=['accuracy'])\n",
    "            total_params = model.count_params()\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "        \n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "            # Train the model\n",
    "            history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(stacked_test, y_test_dl),\n",
    "                                shuffle=False,callbacks=[lr_scheduler])\n",
    "            \n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl,\n",
    "                                                   model = class_model, feature_extractor_model=model, \n",
    "                                                   layer_name='LSTM_FLATTEN')[0]\n",
    "            \n",
    "            accuracy =  accuracy_score(y_test_dl,preds)\n",
    "            precision = precision_score(y_test_dl,preds)\n",
    "            recall = recall_score(y_test_dl,preds)\n",
    "            f1 =  f1_score(y_test_dl,preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "            \n",
    "        # Calculate the average F1 score for the current architecture\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "        # Print the scores for the current architecture\n",
    "        print(f\"Architecture: {architecture}\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print(\"Total Trainable Parameters:\", total_params)\n",
    "        print()\n",
    "        \n",
    "        # Check if the current architecture has a higher average F1 score\n",
    "        # If so, update the best architecture and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_architecture = architecture\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best architecture\n",
    "    print(\"Best Architecture:\")\n",
    "    print(best_architecture)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cnn_temporal(class_model):\n",
    "    \n",
    "#insert the respective architecture configurations to be tested\n",
    "    first_layer_filters = [64,128,256]\n",
    "\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for filter in first_layer_filters:\n",
    "        \n",
    "                \n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "            X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "            \n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "            \n",
    "            # Build the CNN model\n",
    "            model = Sequential()\n",
    "            model.add(Conv1D(filters=filter, kernel_size=5, activation='relu', \n",
    "                             padding='same', input_shape=(num_time_steps, num_features)))\n",
    "            model.add(MaxPooling1D(pool_size=3))\n",
    "            model.add(Flatten(name='CNN_FLATTEN'))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            \n",
    "            initial_learning_rate = 0.001  # Initial learning rate\n",
    "            decay_rate = 0.1  # Decay rate\n",
    "            decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "            \n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate = initial_learning_rate), \n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "            \n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "            \n",
    "            # Train the model\n",
    "            history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size, \n",
    "                                validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "            \n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl, \n",
    "                                                   model = class_model, feature_extractor_model=model, \n",
    "                                                   layer_name='CNN_FLATTEN')[0]\n",
    "            \n",
    "            accuracy =  accuracy_score(y_test_dl,preds)\n",
    "            precision = precision_score(y_test_dl,preds)\n",
    "            recall = recall_score(y_test_dl,preds)\n",
    "            f1 =  f1_score(y_test_dl,preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "            \n",
    "        # Calculate the number of parameters for the current configuration\n",
    "        total_params = model.count_params()\n",
    "        # Calculate the average F1 score for the current configuration\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "        # Print the scores for the current configuration\n",
    "        print(f\"Configuration: filters=({filter})\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print(\"Total Trainable Parameters:\", total_params)\n",
    "        print()\n",
    "\n",
    "        # Check if the current configuration has a higher average F1 score\n",
    "        # If so, update the best configuration and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_configuration = (filter)\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Configuration of filters:\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static + Temporal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "\n",
    "architectures = [\n",
    "\n",
    "    {'hidden_layers': 3, 'units_per_layer': 64},\n",
    "    {'hidden_layers': 3, 'units_per_layer': 128},\n",
    "    {'hidden_layers': 4, 'units_per_layer': 16},\n",
    "    {'hidden_layers': 3, 'units_per_layer': 32},\n",
    "     {'hidden_layers': 4, 'units_per_layer': 32},\n",
    "     {'hidden_layers': 4, 'units_per_layer': 64},\n",
    "     {'hidden_layers': 5, 'units_per_layer': 32},\n",
    "     {'hidden_layers': 6, 'units_per_layer': 32},\n",
    "]\n",
    "\n",
    "best_architecture = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for architecture in architectures:\n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "        X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "        \n",
    "        # Input layers\n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "        temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "        static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "        # RNN layers\n",
    "        rnn_layer = SimpleRNN(architecture['units_per_layer'], \n",
    "                              return_sequences=True, name=f'RNN_LAYER_1')(temporal_input)\n",
    "        for i in range(1, architecture['hidden_layers']):\n",
    "            rnn_layer = SimpleRNN(architecture['units_per_layer'],\n",
    "                                  return_sequences=True, name=f'RNN_LAYER_{i + 1}')(rnn_layer)\n",
    "        rnn_layer = Flatten(name='FLATTEN')(rnn_layer)\n",
    "\n",
    "        # Concatenate RNN layer with static input\n",
    "        rnn_combined = Concatenate(axis=1, name='rnn_CONCAT')([rnn_layer, static_input])\n",
    "        output = Dense(1, activation='sigmoid', name='rnn_OUTPUT_LAYER')(rnn_combined)\n",
    "\n",
    "        model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "        initial_learning_rate = 0.0001\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = 20\n",
    "        batch_size = 64\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "\n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                epochs=epochs, batch_size=batch_size,\n",
    "                                                validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "        y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "        precision = precision_score(y_test_dl, y_pred)\n",
    "        recall = recall_score(y_test_dl, y_pred)\n",
    "        f1 = f1_score(y_test_dl, y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        fold_no = fold_no + 1\n",
    "        \n",
    "    # Calculate the number of parameters for the current architecture\n",
    "    total_params = model.count_params()  \n",
    "    # Calculate the average F1 score for the current architecture\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    # Print the scores for the current architecture\n",
    "    print(f\"Architecture: {architecture['hidden_layers']} RNN layers, {architecture['units_per_layer']} units\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print(\"Total Trainable Parameters:\", total_params)\n",
    "    print()\n",
    "\n",
    "    # Check if the current architecture has a higher average F1 score\n",
    "    # If so, update the best architecture and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_architecture = architecture\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "# Print the best architecture\n",
    "print(\"Best Architecture:\")\n",
    "print(best_architecture)\n",
    "print(\"Best F1 Score:\", best_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "architectures = [\n",
    "    \n",
    "    {'hidden_layers': 2, 'units_per_layer': 256}\n",
    "\n",
    "]\n",
    "\n",
    "best_architecture = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for architecture in architectures:\n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "        X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "        \n",
    "        # Input layers\n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "        temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "        static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "        # LSTM layers\n",
    "        lstm_layer = LSTM(architecture['units_per_layer'], \n",
    "                          return_sequences=True, name=f'LSTM_LAYER_1')(temporal_input)\n",
    "        for i in range(1, architecture['hidden_layers']):\n",
    "            lstm_layer = LSTM(architecture['units_per_layer'], \n",
    "                              return_sequences=True, name=f'LSTM_LAYER_{i + 1}')(lstm_layer)\n",
    "        lstm_layer = Flatten(name='FLATTEN')(lstm_layer)\n",
    "\n",
    "        # Concatenate LSTM layer with static input\n",
    "        LSTM_combined = Concatenate(axis=1, name='LSTM_CONCAT')([lstm_layer, static_input])\n",
    "        output = Dense(1, activation='sigmoid', name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "\n",
    "        model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "        initial_learning_rate = 0.0001\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = 20\n",
    "        batch_size = 64\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "\n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                epochs=epochs, batch_size=batch_size,\n",
    "                                                validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "        y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "        precision = precision_score(y_test_dl, y_pred)\n",
    "        recall = recall_score(y_test_dl, y_pred)\n",
    "        f1 = f1_score(y_test_dl, y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        fold_no = fold_no + 1\n",
    "        \n",
    "    # Calculate the number of parameters for the current architecture\n",
    "    total_params = model.count_params()  \n",
    "    # Calculate the average F1 score for the current architecture\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    # Print the scores for the current architecture\n",
    "    print(f\"Architecture: {architecture['hidden_layers']} LSTM layers, {architecture['units_per_layer']} units\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print(\"Total Trainable Parameters:\", total_params)\n",
    "    print()\n",
    "\n",
    "    # Check if the current architecture has a higher average F1 score\n",
    "    # If so, update the best architecture and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_architecture = architecture\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "# Print the best architecture\n",
    "print(\"Best Architecture:\")\n",
    "print(best_architecture)\n",
    "print(\"Best F1 Score:\", best_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective architecture configurations to be tested\n",
    "\n",
    "first_layer_filters = [4,8]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for filter in first_layer_filters:\n",
    "    \n",
    "            \n",
    "    fold_no = 1\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "        y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "        X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "\n",
    "        num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "        \n",
    "        temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "        static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "        \n",
    "        # CNN layers\n",
    "        cnn_layer = Conv1D(filters=filter, kernel_size=5, activation='relu',padding='same',\n",
    "                           name=f'CNN_LAYER_1')(temporal_input)\n",
    "        cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "        cnn_layer = Conv1D(filters=filter*2, kernel_size=3, activation='relu',padding='same',\n",
    "                           name=f'CNN_LAYER_2')(cnn_layer)\n",
    "        cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "        cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "\n",
    "        # Concatenate CNN layer with static input\n",
    "        cnn_combined = Concatenate(axis=1, name='cnn_CONCAT')([cnn_layer, static_input])\n",
    "        output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(cnn_combined)\n",
    "        \n",
    "        model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "        \n",
    "        initial_learning_rate = 0.001  # Initial learning rate\n",
    "        decay_rate = 0.1  # Decay rate\n",
    "        decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "        batch_size = 64\n",
    "        epochs = 50\n",
    "\n",
    "        def learning_rate_scheduler(epoch):\n",
    "            return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate = initial_learning_rate), \n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no}')\n",
    "    \n",
    "        # Train the model\n",
    "        history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                epochs=epochs, batch_size=batch_size,\n",
    "                                                validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                shuffle=False, callbacks=[lr_scheduler])\n",
    "        \n",
    "        # Plot the loss on train vs validate tests\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "        y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "        precision = precision_score(y_test_dl, y_pred)\n",
    "        recall = recall_score(y_test_dl, y_pred)\n",
    "        f1 = f1_score(y_test_dl, y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "    \n",
    "    # Calculate the average F1 score for the current configuration\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    # Print the scores for the current configuration\n",
    "    print(f\"Configuration: filters={filter, filter*2}\")\n",
    "    print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "    print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "    print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "    print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "    print()\n",
    "\n",
    "    # Check if the current configuration has a higher average F1 score\n",
    "    # If so, update the best configuration and best F1 score\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_configuration = (filter, filter*2)\n",
    "        best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration of filters:\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_rnn_concat(class_model):\n",
    "\n",
    "    #insert the respective architecture configurations to be tested\n",
    "    architectures = [\n",
    "  \n",
    "        {'hidden_layers': 2, 'units_per_layer': 8},\n",
    "        {'hidden_layers': 1, 'units_per_layer': 16},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 8},\n",
    "        {'hidden_layers': 1, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 2, 'units_per_layer': 16},\n",
    "        {'hidden_layers': 3, 'units_per_layer': 16},\n",
    "        {'hidden_layers': 1, 'units_per_layer':64},\n",
    "        {'hidden_layers': 2, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 1, 'units_per_layer': 256},\n",
    "        {'hidden_layers': 1, 'units_per_layer': 128},\n",
    "        {'hidden_layers': 2, 'units_per_layer':64}\n",
    "    ]\n",
    "\n",
    "    best_architecture = None\n",
    "    best_f1_score = 0.0\n",
    "    \n",
    "    for architecture in architectures:\n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "            X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "            \n",
    "            # Input layers\n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "            temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "            static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "            # RNN layers\n",
    "            rnn_layer = SimpleRNN(architecture['units_per_layer'],\n",
    "                                  return_sequences=True, name=f'RNN_LAYER_1')(temporal_input)\n",
    "            for i in range(1, architecture['hidden_layers']):\n",
    "                rnn_layer = SimpleRNN(architecture['units_per_layer'], \n",
    "                                      return_sequences=True, name=f'RNN_LAYER_{i + 1}')(rnn_layer)\n",
    "            rnn_layer = Flatten(name='FLATTEN')(rnn_layer)\n",
    "\n",
    "            # Concatenate RNN layer with static input\n",
    "            RNN_combined = Concatenate(axis=1, name='RNN_CONCAT')([rnn_layer, static_input])\n",
    "            output = Dense(1, activation='sigmoid', name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "\n",
    "            model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "            initial_learning_rate = 0.0001\n",
    "            decay_rate = 0.1\n",
    "            decay_steps = 20\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate),\n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "\n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "            history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                    epochs=epochs, batch_size=batch_size,\n",
    "                                                    validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                    shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction(stacked_train,stacked_test,X_train_static, X_test_static,\n",
    "                                          y_train_dl,y_test_dl,model = class_model, \n",
    "                                          feature_extractor_model = model, layer_name='RNN_CONCAT')[0]\n",
    "            \n",
    "            accuracy = accuracy_score(y_test_dl, preds)\n",
    "            precision = precision_score(y_test_dl, preds)\n",
    "            recall = recall_score(y_test_dl, preds)\n",
    "            f1 = f1_score(y_test_dl, preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "            \n",
    "        # Calculate the average F1 score for the current architecture\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "        # Print the scores for the current architecture\n",
    "        print(f\"Architecture: {architecture}\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print()\n",
    "        \n",
    "        # Check if the current architecture has a higher average F1 score\n",
    "        # If so, update the best architecture and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_architecture = architecture\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best architecture\n",
    "    print(\"Best Architecture:\")\n",
    "    print(best_architecture)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grid_lstm_concat(class_model):\n",
    "\n",
    "   #insert the respective architecture configurations to be tested\n",
    "    architectures = [\n",
    "        {'hidden_layers': 4, 'units_per_layer': 64},\n",
    "        {'hidden_layers': 5, 'units_per_layer': 32},\n",
    "        {'hidden_layers': 6, 'units_per_layer': 32}\n",
    "    ]\n",
    "\n",
    "    best_architecture = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for architecture in architectures:\n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static, \n",
    "            y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "            \n",
    "            # Input layers\n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "            temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "            static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "            # LSTM layers\n",
    "            lstm_layer = LSTM(architecture['units_per_layer'], \n",
    "                              return_sequences=True, name=f'LSTM_LAYER_1')(temporal_input)\n",
    "            for i in range(1, architecture['hidden_layers']):\n",
    "                lstm_layer = LSTM(architecture['units_per_layer'], \n",
    "                                  return_sequences=True, name=f'LSTM_LAYER_{i + 1}')(lstm_layer)\n",
    "            lstm_layer = Flatten(name='FLATTEN')(lstm_layer)\n",
    "\n",
    "            # Concatenate LSTM layer with static input\n",
    "            LSTM_combined = Concatenate(axis=1, name='LSTM_CONCAT')([lstm_layer, static_input])\n",
    "            output = Dense(1, activation='sigmoid', name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "\n",
    "            model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "            initial_learning_rate = 0.0001\n",
    "            decay_rate = 0.1\n",
    "            decay_steps = 20\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate), \n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "\n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "            history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                    epochs=epochs, batch_size=batch_size,\n",
    "                                                    validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                    shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction(stacked_train,stacked_test,X_train_static, X_test_static,\n",
    "                                          y_train_dl,y_test_dl,model = class_model, \n",
    "                                          feature_extractor_model = model, layer_name='LSTM_CONCAT')[0]\n",
    "            \n",
    "            accuracy = accuracy_score(y_test_dl, preds)\n",
    "            precision = precision_score(y_test_dl, preds)\n",
    "            recall = recall_score(y_test_dl, preds)\n",
    "            f1 = f1_score(y_test_dl, preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "            \n",
    "        # Calculate the average F1 score for the current architecture\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "        # Print the scores for the current architecture\n",
    "        print(f\"Architecture: {architecture}\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print()\n",
    "        \n",
    "        # Check if the current architecture has a higher average F1 score\n",
    "        # If so, update the best architecture and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_architecture = architecture\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best architecture\n",
    "    print(\"Best Architecture:\")\n",
    "    print(best_architecture)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cnn_concat(class_model):\n",
    "\n",
    "    first_layer_filters = [32,64,128,256]\n",
    "\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for filter in first_layer_filters:\n",
    "        \n",
    "                \n",
    "        fold_no = 1\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "            X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "            y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "            X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "            \n",
    "            num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "            \n",
    "            temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "            static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "            \n",
    "            # CNN layers\n",
    "            cnn_layer = Conv1D(filters=filter, kernel_size=5,\n",
    "                               activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "            cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "            cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "\n",
    "            # Concatenate CNN layer with static input\n",
    "            cnn_combined = Concatenate(axis=1, name='CNN_CONCAT')([cnn_layer, static_input])\n",
    "            output = Dense(1, activation='sigmoid', name='CNN_OUTPUT_LAYER')(cnn_combined)\n",
    "            \n",
    "            model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "            \n",
    "            initial_learning_rate = 0.001  # Initial learning rate\n",
    "            decay_rate = 0.1  # Decay rate\n",
    "            decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "            batch_size = 64\n",
    "            epochs = 50\n",
    "\n",
    "            def learning_rate_scheduler(epoch):\n",
    "                return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "            \n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate = initial_learning_rate), \n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            \n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no}')\n",
    "            \n",
    "            lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "        \n",
    "            # Train the model\n",
    "            history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                    epochs=epochs, batch_size=batch_size,\n",
    "                                                    validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                    shuffle=False, callbacks=[lr_scheduler])\n",
    "            \n",
    "            # Plot the loss on train vs validate tests\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            preds = classifier_prediction(stacked_train,stacked_test,X_train_static, X_test_static,\n",
    "                                          y_train_dl,y_test_dl,model = class_model, \n",
    "                                          feature_extractor_model = model, layer_name='CNN_CONCAT')[0]\n",
    "                \n",
    "            accuracy = accuracy_score(y_test_dl, preds)\n",
    "            precision = precision_score(y_test_dl, preds)\n",
    "            recall = recall_score(y_test_dl, preds)\n",
    "            f1 = f1_score(y_test_dl, preds)\n",
    "\n",
    "            accuracy_scores.append(accuracy)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "            fold_no = fold_no + 1\n",
    "        \n",
    "        # Calculate the average F1 score for the current configuration\n",
    "        average_f1_score = np.mean(f1_scores)\n",
    "        total_params = model.count_params()\n",
    "\n",
    "        # Print the scores for the current configuration\n",
    "        print(f\"Configuration: filters={filter}\")\n",
    "        print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "        print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "        print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "        print(\"Total Trainable Parameters of CNN:\", total_params)\n",
    "        print()\n",
    "\n",
    "        # Check if the current configuration has a higher average F1 score\n",
    "        # If so, update the best configuration and best F1 score\n",
    "        if average_f1_score > best_f1_score:\n",
    "            best_configuration = (filter)\n",
    "            best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Configuration of filters:\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Curves for Optimal Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excel file contains all the tables, so I retrieve teh tables and then I plot the elbow curves \n",
    "# 1 file for RNN, LSTM and 1 file for CNN\n",
    "\n",
    "paths = [\"C:/Users/diama/Downloads/elbowplots_lstm_rnn.xlsx\", \"C:/Users/diama/Downloads/elbowplots_cnn.xlsx\"]\n",
    "\n",
    "for excel_file_path in paths:\n",
    "    \n",
    "    with pd.ExcelFile(excel_file_path) as xls:\n",
    "        sheet_names = xls.sheet_names\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "    \n",
    "        data_excel = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "        data_excel = data_excel.rename(columns={'F1 Score': 'F1_score'})\n",
    "        num_parameters = data_excel['Num_parameters']\n",
    "        f1_scores = data_excel['F1_score']\n",
    "            \n",
    "        # Create the elbow plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(num_parameters, f1_scores, marker='o')\n",
    "        \n",
    "        # Find the index of the best architecture\n",
    "        best_index = data_excel['F1_score'].idxmax()\n",
    "        \n",
    "        # Highlight the point of the best architecture with a red dot\n",
    "        plt.scatter(num_parameters[best_index], f1_scores[best_index], c='r', s=60)\n",
    "        \n",
    "            \n",
    "        # Mark the point with the highest F1 score\n",
    "        best_arch = data_excel.iloc[best_index]\n",
    "        \n",
    "        if \"cnn\" in excel_file_path:\n",
    "            plt.text(0.97, 0.03, f'Best: Layers={int(best_arch[\"Layers\"])}, Filters={best_arch[\"Filters\"]}',\n",
    "             transform=plt.gca().transAxes, color='black', fontsize=10,\n",
    "             verticalalignment='bottom', horizontalalignment='right')\n",
    "        else:\n",
    "            plt.text(0.97, 0.03, f'Best: Layers={int(best_arch[\"Layers\"])}, Units={int(best_arch[\"Units\"])}',\n",
    "                transform=plt.gca().transAxes, color='black', fontsize=10,\n",
    "                verticalalignment='bottom', horizontalalignment='right')\n",
    "            \n",
    "        plt.xlabel('Number of Parameters')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Elbow Plot for {sheet_name}: F1 Score vs Number of Parameters')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer, Learning Rate, Batch Size Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "initial_learning_rates = [0.00001]\n",
    "optimizers = ['rmsprop']\n",
    "batches = [32]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "            \n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "        \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                \n",
    "                # Build the RNN model\n",
    "                model = Sequential()\n",
    "                model.add(SimpleRNN(32, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "                model.add(SimpleRNN(32, return_sequences=True))\n",
    "                model.add(SimpleRNN(32, return_sequences=True))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "                \n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                \n",
    "                \n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "            \n",
    "                batch_size = bs\n",
    "                \n",
    "                # Train the model\n",
    "                history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                    validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                \n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                \n",
    "                y_pred_probs = model.predict(stacked_test)\n",
    "                y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "                \n",
    "                accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "                precision = precision_score(y_test_dl,y_pred)\n",
    "                recall = recall_score(y_test_dl,y_pred)\n",
    "                f1 =  f1_score(y_test_dl,y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "                \n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            print()\n",
    "        \n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "initial_learning_rates = [0.001,0.00001]\n",
    "optimizers = ['adam']\n",
    "batches = [64]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "            \n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "        \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static,\n",
    "                y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                \n",
    "                # Build the LSTM model\n",
    "                model = Sequential()\n",
    "                model.add(LSTM(64, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "                \n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                \n",
    "                \n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "            \n",
    "                batch_size = bs\n",
    "                \n",
    "                # Train the model\n",
    "                history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                    validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                \n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                \n",
    "                y_pred_probs = model.predict(stacked_test)\n",
    "                y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "                \n",
    "                accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "                precision = precision_score(y_test_dl,y_pred)\n",
    "                recall = recall_score(y_test_dl,y_pred)\n",
    "                f1 =  f1_score(y_test_dl,y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "                \n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            print()\n",
    "        \n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "initial_learning_rates = [0.001]\n",
    "optimizers = ['adam']\n",
    "batches = [64]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "            \n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "        \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static, \n",
    "                y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                \n",
    "                # Build the CNN model\n",
    "                model = Sequential()\n",
    "                model.add(Conv1D(filters=4, kernel_size=5, \n",
    "                                 activation='relu',padding='same', input_shape=(num_time_steps, num_features)))\n",
    "                model.add(MaxPooling1D(pool_size=3))\n",
    "                model.add(Conv1D(filters=8, padding='same', kernel_size=3, activation='relu'))\n",
    "                model.add(MaxPooling1D(pool_size=2))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "                \n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                \n",
    "                \n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "            \n",
    "                batch_size = bs\n",
    "                \n",
    "                # Train the model\n",
    "                history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                    validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                \n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                \n",
    "                y_pred_probs = model.predict(stacked_test)\n",
    "                y_pred = (y_pred_probs>=0.5).astype(int)    \n",
    "                \n",
    "                accuracy =  accuracy_score(y_test_dl,y_pred)\n",
    "                precision = precision_score(y_test_dl,y_pred)\n",
    "                recall = recall_score(y_test_dl,y_pred)\n",
    "                f1 =  f1_score(y_test_dl,y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "                \n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            print()\n",
    "        \n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_rnn_temporal_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.001,0.0001,0.00001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [32,128]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "                \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                    \n",
    "                    # Build the RNN model\n",
    "                    model = Sequential()\n",
    "                    model.add(SimpleRNN(32, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "                    model.add(Flatten(name='RNN_FLATTEN'))\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "                    \n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                    \n",
    "                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                    \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                \n",
    "                    batch_size = bs\n",
    "                \n",
    "                    # Train the model\n",
    "                    history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                        validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                    \n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl, \n",
    "                                                           model = class_model, feature_extractor_model=model, \n",
    "                                                           layer_name='RNN_FLATTEN')[0]\n",
    "                    \n",
    "                    accuracy =  accuracy_score(y_test_dl,preds)\n",
    "                    precision = precision_score(y_test_dl,preds)\n",
    "                    recall = recall_score(y_test_dl,preds)\n",
    "                    f1 =  f1_score(y_test_dl,preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                \n",
    "                    \n",
    "                # Calculate the average F1 score for the current configuration\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "            \n",
    "            # Print the best configuration\n",
    "            print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "            print(best_configuration)\n",
    "            print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_temporal_optim(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_lstm_temporal_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.001, 0.00001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [64]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "        \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "                    \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                    \n",
    "                    # Build the LSTM model\n",
    "                    model = Sequential()\n",
    "                    model.add(LSTM(16, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "                    model.add(Flatten(name='LSTM_FLATTEN'))\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "                    \n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                    \n",
    "                    \n",
    "                    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate),\n",
    "                                  metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                    \n",
    "                    batch_size = bs                \n",
    "                    \n",
    "                    # Train the model\n",
    "                    history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                        validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                    \n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl,\n",
    "                                                           model = class_model, feature_extractor_model=model,\n",
    "                                                           layer_name='LSTM_FLATTEN')[0]\n",
    "                    \n",
    "                    accuracy =  accuracy_score(y_test_dl,preds)\n",
    "                    precision = precision_score(y_test_dl,preds)\n",
    "                    recall = recall_score(y_test_dl,preds)\n",
    "                    f1 =  f1_score(y_test_dl,preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                    \n",
    "                # Calculate the average F1 score for the current configuration\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "                print()\n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Architecture:\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_temporal_optim(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cnn_temporal_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [64]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "                \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "            \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl,y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "                    \n",
    "                    # Build the CNN model\n",
    "                    model = Sequential()\n",
    "                    model.add(Conv1D(filters=16, kernel_size=5,\n",
    "                                     activation='relu',padding='same', input_shape=(num_time_steps, num_features)))\n",
    "                    model.add(MaxPooling1D(pool_size=3))\n",
    "                    model.add(Conv1D(filters=32, padding='same', kernel_size=3, activation='relu'))\n",
    "                    model.add(MaxPooling1D(pool_size=2))\n",
    "                    model.add(Flatten(name = 'CNN_FLATTEN'))\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "                    \n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "                    \n",
    "                  \n",
    "                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                    \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                \n",
    "                    batch_size = bs\n",
    "                \n",
    "                    # Train the model\n",
    "                    history = model.fit(stacked_train, y_train_dl, epochs=epochs, batch_size=batch_size,\n",
    "                                        validation_data=(stacked_test, y_test_dl),shuffle=False,callbacks=[lr_scheduler])\n",
    "                    \n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction_temporal(stacked_train,stacked_test, y_train_dl, \n",
    "                                                           model = class_model, feature_extractor_model=model,\n",
    "                                                           layer_name='CNN_FLATTEN')[0]\n",
    "                    \n",
    "                    accuracy =  accuracy_score(y_test_dl,preds)\n",
    "                    precision = precision_score(y_test_dl,preds)\n",
    "                    recall = recall_score(y_test_dl,preds)\n",
    "                    f1 =  f1_score(y_test_dl,preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                    \n",
    "                # Calculate the average F1 score for the current configuration\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "            \n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "            \n",
    "            # Print the best configuration\n",
    "            print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "            print(best_configuration)\n",
    "            print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_temporal_optim(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static + Temporal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "initial_learning_rates = [0.001, 0.0001, 0.00001]\n",
    "optimizers = ['rmsprop','adam']\n",
    "batches = [128]\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "\n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "    \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test, X_test_static,\n",
    "                y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                # Input layers\n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                # RNN layers\n",
    "                rnn_layer = SimpleRNN(64, return_sequences=True, name=f'RNN_LAYER_1')(temporal_input)\n",
    "                rnn_layer = SimpleRNN(64, return_sequences=True, name=f'RNN_LAYER_2')(rnn_layer)\n",
    "                rnn_layer = SimpleRNN(64, return_sequences=True, name=f'RNN_LAYER_3')(rnn_layer)\n",
    "                rnn_layer = Flatten(name='FLATTEN')(rnn_layer)\n",
    "\n",
    "                # Concatenate RNN layer with static input\n",
    "                rnn_combined = Concatenate(axis=1, name='rnn_CONCAT')([rnn_layer, static_input])\n",
    "                output = Dense(1, activation='sigmoid', name='RNN_OUTPUT_LAYER')(rnn_combined)\n",
    "\n",
    "                model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "                \n",
    "                batch_size = bs\n",
    "                history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                        epochs=epochs, batch_size=batch_size,\n",
    "                                                        validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                        shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                # Make predictions on the test set\n",
    "                y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "                y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "                accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "                precision = precision_score(y_test_dl, y_pred)\n",
    "                recall = recall_score(y_test_dl, y_pred)\n",
    "                f1 = f1_score(y_test_dl, y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "\n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "\n",
    "initial_learning_rates = [0.001, 0.0001, 0.00001]\n",
    "optimizers = ['rmsprop']\n",
    "batches = [64]\n",
    "\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "\n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "    \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "                X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                # Input layers\n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                # LSTM layers\n",
    "                lstm_layer = LSTM(64, return_sequences=True, name=f'LSTM_LAYER_1')(temporal_input)\n",
    "                lstm_layer = LSTM(64, return_sequences=True, name=f'LSTM_LAYER_2')(lstm_layer)\n",
    "                lstm_layer = LSTM(64, return_sequences=True, name=f'LSTM_LAYER_3')(lstm_layer)\n",
    "                lstm_layer = LSTM(64, return_sequences=True, name=f'LSTM_LAYER_4')(lstm_layer)\n",
    "                lstm_layer = Flatten(name='FLATTEN')(lstm_layer)\n",
    "\n",
    "                # Concatenate LSTM layer with static input\n",
    "                lstm_combined = Concatenate(axis=1, name='LSTM_CONCAT')([lstm_layer, static_input])\n",
    "                output = Dense(1, activation='sigmoid', name='LSTM_OUTPUT_LAYER')(lstm_combined)\n",
    "\n",
    "                model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "\n",
    "                batch_size = bs\n",
    "\n",
    "                history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                        epochs=epochs, batch_size=batch_size,\n",
    "                                                        validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                        shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                # Make predictions on the test set\n",
    "                y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "                y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "                accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "                precision = precision_score(y_test_dl, y_pred)\n",
    "                recall = recall_score(y_test_dl, y_pred)\n",
    "                f1 = f1_score(y_test_dl, y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "\n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the respective configurations to be tested\n",
    "\n",
    "initial_learning_rates = [0.001, 0.0001, 0.00001]\n",
    "optimizers = ['rmsprop','adam']\n",
    "batches = [128]\n",
    "\n",
    "\n",
    "best_configuration = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "\n",
    "for lr in initial_learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for bs in batches:\n",
    "\n",
    "            fold_no = 1\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "    \n",
    "            for train_index, test_index in kf.split(X_train, y_train):\n",
    "                \n",
    "                X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                \n",
    "                # Input layers\n",
    "                num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                # CNN layers\n",
    "                cnn_layer = Conv1D(filters=256, kernel_size=5, \n",
    "                                   activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "                cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "                cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "\n",
    "                # Concatenate CNN layer with static input\n",
    "                cnn_combined = Concatenate(axis=1, name='cnn_CONCAT')([cnn_layer, static_input])\n",
    "                output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(cnn_combined)\n",
    "\n",
    "                model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                initial_learning_rate = lr  # Initial learning rate\n",
    "                decay_rate = 0.1  # Decay rate\n",
    "                decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                epochs = 50\n",
    "\n",
    "                def learning_rate_scheduler(epoch):\n",
    "                    return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                \n",
    "                if optimizer == 'rmsprop':\n",
    "                    optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                elif optimizer == 'adam':\n",
    "                    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                \n",
    "                lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                \n",
    "                print('------------------------------------------------------------------------')\n",
    "                print(f'Training for fold {fold_no}')\n",
    "                \n",
    "                batch_size = bs\n",
    "                \n",
    "                # Train DL model\n",
    "                history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                        epochs=epochs, batch_size=batch_size,\n",
    "                                                        validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                        shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                # Plot the loss on train vs validate tests\n",
    "                plt.plot(history.history['loss'], label='Train Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                # Make predictions on the test set\n",
    "                y_pred_probs = model.predict([stacked_test, X_test_static])\n",
    "                y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "                accuracy = accuracy_score(y_test_dl, y_pred)\n",
    "                precision = precision_score(y_test_dl, y_pred)\n",
    "                recall = recall_score(y_test_dl, y_pred)\n",
    "                f1 = f1_score(y_test_dl, y_pred)\n",
    "\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "                fold_no = fold_no + 1\n",
    "            \n",
    "            \n",
    "            # Calculate the average F1 score for the current configuration\n",
    "            average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            # Print the scores for the current configuration\n",
    "            print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "            print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "            print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "            print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "            print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "\n",
    "            # Check if the current configuration has a higher average F1 score\n",
    "            # If so, update the best configuration and best F1 score\n",
    "            if average_f1_score > best_f1_score:\n",
    "                best_configuration = (lr, optimizer,batch_size)\n",
    "                best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "print(best_configuration)\n",
    "print(\"Best F1 Score:\", best_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_rnn_concat_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.0001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [128]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "        \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "                    \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    # Input layers\n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                    temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                    static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                    # RNN layers\n",
    "                    rnn_layer = SimpleRNN(32, return_sequences=True, name=f'RNN_LAYER_1')(temporal_input)\n",
    "                    rnn_layer = SimpleRNN(32, return_sequences=True, name=f'RNN_LAYER_2')(rnn_layer)\n",
    "                    rnn_layer = Flatten(name='FLATTEN')(rnn_layer)\n",
    "\n",
    "                    # Concatenate RNN layer with static input\n",
    "                    RNN_combined = Concatenate(axis=1, name='RNN_CONCAT')([rnn_layer, static_input])\n",
    "                    output = Dense(1, activation='sigmoid', name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "\n",
    "                    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                    \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                    \n",
    "                    batch_size = bs\n",
    "\n",
    "                    #Train DL model\n",
    "                    history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                            epochs=epochs, batch_size=batch_size,\n",
    "                                                            validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                            shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction(stacked_train,stacked_test,X_train_static, X_test_static,\n",
    "                                                  y_train_dl,y_test_dl,model = class_model, \n",
    "                                                  feature_extractor_model = model, layer_name='RNN_CONCAT')[0]\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_test_dl, preds)\n",
    "                    precision = precision_score(y_test_dl, preds)\n",
    "                    recall = recall_score(y_test_dl, preds)\n",
    "                    f1 = f1_score(y_test_dl, preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                    \n",
    "                # Calculate the average F1 score for the current configuration\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "             \n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rnn_concat_optim(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_lstm_concat_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.001, 0.00001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [64]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "        \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "                    \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test,\n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    # Input layers\n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                    temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                    static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                    # LSTM layers\n",
    "                    lstm_layer = LSTM(128, return_sequences=True, name='LSTM_LAYER_1')(temporal_input)\n",
    "                    lstm_layer = LSTM(128, return_sequences=True, name='LSTM_LAYER_2')(lstm_layer)\n",
    "                    lstm_layer = Flatten(name='FLATTEN')(lstm_layer)\n",
    "\n",
    "                    # Concatenate LSTM layer with static input\n",
    "                    LSTM_combined = Concatenate(axis=1, name='LSTM_CONCAT')([lstm_layer, static_input])\n",
    "                    output = Dense(1, activation='sigmoid', name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "\n",
    "                    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                    \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                    \n",
    "                    batch_size = bs\n",
    "\n",
    "                    #Train DL model\n",
    "                    history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                            epochs=epochs, batch_size=batch_size,\n",
    "                                                            validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                            shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction(stacked_train,stacked_test,X_train_static,\n",
    "                                                  X_test_static,y_train_dl,y_test_dl,model = class_model, \n",
    "                                                  feature_extractor_model = model, layer_name='LSTM_CONCAT')[0]\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_test_dl, preds)\n",
    "                    precision = precision_score(y_test_dl, preds)\n",
    "                    recall = recall_score(y_test_dl, preds)\n",
    "                    f1 = f1_score(y_test_dl, preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                    \n",
    "                # Calculate the average F1 score for the current architecture\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "             \n",
    "\n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lstm_concat_optim(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cnn_concat_optim(class_model):\n",
    "    \n",
    "    #insert the respective configurations to be tested\n",
    "    initial_learning_rates = [0.001]\n",
    "    optimizers = ['adam']\n",
    "    batches = [64]\n",
    "\n",
    "    best_configuration = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for lr in initial_learning_rates:\n",
    "        for optimizer in optimizers:\n",
    "            for bs in batches:\n",
    "                \n",
    "                fold_no = 1\n",
    "                accuracy_scores = []\n",
    "                precision_scores = []\n",
    "                recall_scores = []\n",
    "                f1_scores = []\n",
    "        \n",
    "                for train_index, test_index in kf.split(X_train, y_train):\n",
    "                    \n",
    "                    X_train_dl, X_test_dl = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "                    y_train_dl, y_test_dl = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                    stacked_train, X_train_static, y_train_dl, stacked_test, \n",
    "                    X_test_static, y_test_dl = preprocess_data(X_train_dl, y_train_dl, X_test_dl, y_test_dl)\n",
    "                    \n",
    "                    # Input layers\n",
    "                    num_time_steps, num_features = stacked_train.shape[1], stacked_train.shape[2]\n",
    "\n",
    "                    temporal_input = Input(shape=(num_time_steps, num_features), name='TEMPORAL_INPUT')\n",
    "                    static_input = Input(shape=(X_train_static.shape[1]), name='STATIC_INPUT')\n",
    "\n",
    "                    # CNN layers\n",
    "                    cnn_layer = Conv1D(filters=8, kernel_size=5,\n",
    "                                       activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "                    cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "                    cnn_layer = Conv1D(filters=16, kernel_size=3,\n",
    "                                       activation='relu',padding='same', name=f'CNN_LAYER_2')(cnn_layer)\n",
    "                    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "                    cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "\n",
    "                    # Concatenate CNN layer with static input\n",
    "                    cnn_combined = Concatenate(axis=1, name='CNN_CONCAT')([cnn_layer, static_input])\n",
    "                    output = Dense(1, activation='sigmoid', name='CNN_OUTPUT_LAYER')(cnn_combined)\n",
    "                    \n",
    "                    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "\n",
    "                    initial_learning_rate = lr  # Initial learning rate\n",
    "                    decay_rate = 0.1  # Decay rate\n",
    "                    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "                    epochs = 50\n",
    "\n",
    "                    def learning_rate_scheduler(epoch):\n",
    "                        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "                    \n",
    "                    if optimizer == 'rmsprop':\n",
    "                        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "                    elif optimizer == 'adam':\n",
    "                        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                    \n",
    "                    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "                    \n",
    "                    print('------------------------------------------------------------------------')\n",
    "                    print(f'Training for fold {fold_no}')\n",
    "                    \n",
    "                    batch_size = bs\n",
    "\n",
    "                    # Train DL model\n",
    "                    history = model.fit([stacked_train, X_train_static], y_train_dl,\n",
    "                                                            epochs=epochs, batch_size=batch_size,\n",
    "                                                            validation_data=([stacked_test, X_test_static], y_test_dl),\n",
    "                                                            shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "                    # Plot the loss on train vs validate tests\n",
    "                    plt.plot(history.history['loss'], label='Train Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    preds = classifier_prediction(stacked_train,stacked_test,X_train_static,\n",
    "                                                  X_test_static,y_train_dl,y_test_dl,model = class_model,\n",
    "                                                  feature_extractor_model = model, layer_name='CNN_CONCAT')[0]\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_test_dl, preds)\n",
    "                    precision = precision_score(y_test_dl, preds)\n",
    "                    recall = recall_score(y_test_dl, preds)\n",
    "                    f1 = f1_score(y_test_dl, preds)\n",
    "\n",
    "                    accuracy_scores.append(accuracy)\n",
    "                    precision_scores.append(precision)\n",
    "                    recall_scores.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "                    \n",
    "                    fold_no = fold_no + 1\n",
    "                    \n",
    "                # Calculate the average F1 score for the current configuration\n",
    "                average_f1_score = np.mean(f1_scores)\n",
    "             \n",
    "\n",
    "                # Print the scores for the current configuration\n",
    "                print(f\"Configuration: Learning Rate={lr}, Optimizer={optimizer.get_config()['name']},Batch Size ={bs}\")\n",
    "                print('Accuracy: %.3f (+/- %.3f)' % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "                print('Precision: %.3f (+/- %.3f)' % (np.mean(precision_scores), np.std(precision_scores)))\n",
    "                print('Recall: %.3f (+/- %.3f)' % (np.mean(recall_scores), np.std(recall_scores)))\n",
    "                print('F1 score: %.3f (+/- %.3f)' % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "                \n",
    "                # Check if the current configuration has a higher average F1 score\n",
    "                # If so, update the best configuration and best F1 score\n",
    "                if average_f1_score > best_f1_score:\n",
    "                    best_configuration = (lr, optimizer,batch_size)\n",
    "                    best_f1_score = average_f1_score\n",
    "\n",
    "\n",
    "    # Print the best configuration\n",
    "    print(\"Best Configuration (Learning Rate, Optimizer, Batch Size):\")\n",
    "    print(best_configuration)\n",
    "    print(\"Best F1 Score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat_optim(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat_optim(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cnn_concat_optim(XGBClassifier())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing on the train - test set before prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temporal, X_train_static, y_train_preprocessed, X_test_temporal,\n",
    "X_test_static, y_test_preprocessed = preprocess_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_steps, num_features = X_train_temporal.shape[1], X_train_temporal.shape[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized temporal models (Dense layer)\n",
    "\n",
    "def RNN_Temporal():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32,return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(SimpleRNN(32,return_sequences = True))\n",
    "    model.add(SimpleRNN(32,return_sequences = True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def LSTM_Temporal():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def CNN_Temporal():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=4, kernel_size=5, padding='same', activation='relu',\n",
    "                     input_shape=(num_time_steps, num_features)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=8, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized temporal models (with ML in FCL)\n",
    "\n",
    "def RNN_Temporal_LR():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(SimpleRNN(32, return_sequences=True))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def LSTM_Temporal_LR():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(16, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def CNN_Temporal_LR():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=4, kernel_size=5, padding='same', activation='relu', \n",
    "                     input_shape=(num_time_steps, num_features)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=8, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def RNN_Temporal_RF():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def LSTM_Temporal_RF():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def CNN_Temporal_RF():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=8, kernel_size=5, padding='same', activation='relu',\n",
    "                     input_shape=(num_time_steps, num_features)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def RNN_Temporal_XGB():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(16, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def LSTM_Temporal_XGB():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(16, return_sequences=True, input_shape=(num_time_steps, num_features)))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def CNN_Temporal_XGB():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=16, kernel_size=5, padding='same', activation='relu', \n",
    "                     input_shape=(num_time_steps, num_features)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten(name='FLATTEN'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and evaluating on test set for optimized configurations (temporal - dense)\n",
    "\n",
    "def temporal_dense_evaluation(X_train, y_train, X_test, y_test, batch_size, optimizer, \n",
    "                              initial_learning_rate, model):\n",
    "    \n",
    "    decay_rate = 0.1  # Decay rate\n",
    "    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "    epochs = 50\n",
    "\n",
    "    def learning_rate_scheduler(epoch):\n",
    "        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "    \n",
    "    if optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test), shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "    # Plot the loss on train vs validate tests\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on the testing set\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    test_accuracy =  accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred)\n",
    "    test_recall = recall_score(y_test, y_pred)\n",
    "    test_f1 =  f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    y_train_pred_probs = model.predict(X_train)\n",
    "    y_train_pred = (y_train_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "   \n",
    "    print(f\"Testing Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % test_accuracy)\n",
    "    print(\"Precision: %.3f\" % test_precision)\n",
    "    print(\"Recall: %.3f\" % test_recall)\n",
    "    print(\"F1 score: %.3f\" % test_f1)\n",
    "  \n",
    "    print('-----------------------------------')\n",
    "    print(f\"Training Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Precision: %.3f\" % train_precision)\n",
    "    print(\"Recall: %.3f\" % train_recall)\n",
    "    print(\"F1 score: %.3f\" % train_f1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and evaluating on test set for optimized configurations (temporal - ML)\n",
    "\n",
    "def temporal_ML_evaluation(X_train, y_train, X_test, y_test, batch_size, optimizer, \n",
    "                           initial_learning_rate, model, ml_model):\n",
    "    \n",
    "    decay_rate = 0.1  # Decay rate\n",
    "    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "    epochs = 50\n",
    "\n",
    "    def learning_rate_scheduler(epoch):\n",
    "        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "    \n",
    "    if optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_data=(X_test, y_test), shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "    # Plot the loss on train vs validate tests\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    preds,preds_train = classifier_prediction_temporal(X_train,X_test, y_train, \n",
    "                                                       model = ml_model, feature_extractor_model= model, \n",
    "                                                       layer_name='FLATTEN')\n",
    "    \n",
    "    \n",
    "    # Evaluate on the testing set\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    test_precision = precision_score(y_test, preds)\n",
    "    test_recall = recall_score(y_test, preds)\n",
    "    test_f1 =  f1_score(y_test, preds)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    train_accuracy = accuracy_score(y_train, preds_train)\n",
    "    train_precision = precision_score(y_train, preds_train)\n",
    "    train_recall = recall_score(y_train, preds_train)\n",
    "    train_f1 = f1_score(y_train, preds_train)\n",
    "   \n",
    "    print(f\"Testing Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % test_accuracy)\n",
    "    print(\"Precision: %.3f\" % test_precision)\n",
    "    print(\"Recall: %.3f\" % test_recall)\n",
    "    print(\"F1 score: %.3f\" % test_f1)\n",
    "  \n",
    "    print('-----------------------------------')\n",
    "    print(f\"Training Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Precision: %.3f\" % train_precision)\n",
    "    print(\"Recall: %.3f\" % train_recall)\n",
    "    print(\"F1 score: %.3f\" % train_f1)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_dense_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=32,\n",
    "                          optimizer='rmsprop',initial_learning_rate=0.0001, model=RNN_Temporal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_dense_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                          optimizer='adam',initial_learning_rate=0.0001, model=LSTM_Temporal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Temporal (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_dense_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                          optimizer='adam',initial_learning_rate=0.001, model=CNN_Temporal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=128,\n",
    "                       optimizer='rmsprop',initial_learning_rate=0.0001, \n",
    "                       model=RNN_Temporal_LR(),ml_model = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='adam',initial_learning_rate=0.001, \n",
    "                       model=RNN_Temporal_RF(),ml_model = RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='adam',initial_learning_rate=0.0001, \n",
    "                       model=RNN_Temporal_XGB(),ml_model = XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=32,\n",
    "                       optimizer='adam',initial_learning_rate=0.0001, \n",
    "                       model=LSTM_Temporal_LR(),ml_model = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='rmsprop',initial_learning_rate=0.001,\n",
    "                       model=LSTM_Temporal_RF(),ml_model = RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=32,\n",
    "                       optimizer='rmsprop',initial_learning_rate=0.0001, \n",
    "                       model=LSTM_Temporal_XGB(),ml_model = XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Temporal + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='adam',initial_learning_rate=0.001, \n",
    "                       model=CNN_Temporal_LR(),ml_model = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='adam',initial_learning_rate=0.001, \n",
    "                       model=CNN_Temporal_RF(),ml_model = RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_ML_evaluation(X_train_temporal, y_train_preprocessed,X_test_temporal, y_test_preprocessed,batch_size=64,\n",
    "                       optimizer='adam',initial_learning_rate=0.001, \n",
    "                       model=CNN_Temporal_XGB(),ml_model = XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static + Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Concat models (Dense layer)\n",
    "\n",
    "def RNN_Concat():\n",
    "    rnn_layer = SimpleRNN(64,return_sequences=True, name = 'RNN_LAYER_1')(temporal_input)\n",
    "    rnn_layer = SimpleRNN(64,return_sequences=True, name = 'RNN_LAYER_2')(rnn_layer)\n",
    "    rnn_layer = SimpleRNN(64,return_sequences=True, name = 'RNN_LAYER_3')(rnn_layer)\n",
    "    rnn_layer = Flatten(name = 'FLATTEN')(rnn_layer)\n",
    "    RNN_combined = Concatenate(axis=1, name ='RNN_CONCAT')([rnn_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def LSTM_Concat():\n",
    "    lstm_layer = LSTM(64,return_sequences=True, name = 'LSTM_LAYER_1')(temporal_input)\n",
    "    lstm_layer = LSTM(64,return_sequences=True, name = 'LSTM_LAYER_2')(lstm_layer)\n",
    "    lstm_layer = LSTM(64,return_sequences=True, name = 'LSTM_LAYER_3')(lstm_layer)\n",
    "    lstm_layer = LSTM(64,return_sequences=True, name = 'LSTM_LAYER_4')(lstm_layer)\n",
    "    lstm_layer = Flatten(name = 'FLATTEN')(lstm_layer)\n",
    "    LSTM_combined = Concatenate(axis=1, name ='LSTM_CONCAT')([lstm_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def CNN_Concat():\n",
    "    cnn_layer = Conv1D(filters=256, kernel_size=5, activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "    cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "    cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "    CNN_combined = Concatenate(axis=1, name='cnn_CONCAT')([cnn_layer, static_input])\n",
    "    output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(CNN_combined)\n",
    "    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Concat models (with ML in FCL)\n",
    "\n",
    "def RNN_Concat_LR():\n",
    "    rnn_layer = SimpleRNN(32,return_sequences=True, name = 'RNN_LAYER_1')(temporal_input)\n",
    "    rnn_layer = SimpleRNN(32,return_sequences=True, name = 'RNN_LAYER_2')(rnn_layer)\n",
    "    rnn_layer = Flatten(name = 'FLATTEN')(rnn_layer)\n",
    "    RNN_combined = Concatenate(axis=1, name ='CONCAT')([rnn_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def LSTM_Concat_LR():\n",
    "    lstm_layer = LSTM(32,return_sequences=True, name = 'LSTM_LAYER_1')(temporal_input)\n",
    "    lstm_layer = LSTM(32,return_sequences=True, name = 'LSTM_LAYER_2')(lstm_layer)\n",
    "    lstm_layer = Flatten(name = 'FLATTEN')(lstm_layer)\n",
    "    LSTM_combined = Concatenate(axis=1, name ='CONCAT')([lstm_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def CNN_Concat_LR():\n",
    "    cnn_layer = Conv1D(filters=4, kernel_size=5, activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "    cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "    cnn_layer = Conv1D(filters=8, kernel_size=3, activation='relu',padding='same', name=f'CNN_LAYER_2')(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "    CNN_combined = Concatenate(axis=1, name='CONCAT')([cnn_layer, static_input])\n",
    "    output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(CNN_combined)\n",
    "    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "    return model\n",
    "\n",
    "def RNN_Concat_RF():\n",
    "    rnn_layer = SimpleRNN(16,return_sequences=True, name = 'RNN_LAYER_1')(temporal_input)\n",
    "    rnn_layer = Flatten(name = 'FLATTEN')(rnn_layer)\n",
    "    RNN_combined = Concatenate(axis=1, name ='CONCAT')([rnn_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def LSTM_Concat_RF():\n",
    "    lstm_layer = LSTM(16,return_sequences=True, name = 'LSTM_LAYER_1')(temporal_input)\n",
    "    lstm_layer = Flatten(name = 'FLATTEN')(lstm_layer)\n",
    "    LSTM_combined = Concatenate(axis=1, name ='CONCAT')([lstm_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def CNN_Concat_RF():\n",
    "    cnn_layer = Conv1D(filters=8, kernel_size=5, activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "    cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "    cnn_layer = Conv1D(filters=16, kernel_size=3, activation='relu',padding='same', name=f'CNN_LAYER_2')(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "    CNN_combined = Concatenate(axis=1, name='CONCAT')([cnn_layer, static_input])\n",
    "    output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(CNN_combined)\n",
    "    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "    return model\n",
    "\n",
    "def RNN_Concat_XGB():\n",
    "    rnn_layer = SimpleRNN(128,return_sequences=True, name = 'RNN_LAYER_1')(temporal_input)\n",
    "    rnn_layer = SimpleRNN(128,return_sequences=True, name = 'RNN_LAYER_2')(rnn_layer)\n",
    "    rnn_layer = Flatten(name = 'FLATTEN')(rnn_layer)\n",
    "    RNN_combined = Concatenate(axis=1, name ='CONCAT')([rnn_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='RNN_OUTPUT_LAYER')(RNN_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def LSTM_Concat_XGB():\n",
    "    lstm_layer = LSTM(32,return_sequences=True, name = 'LSTM_LAYER_1')(temporal_input)\n",
    "    lstm_layer = LSTM(32,return_sequences=True, name = 'LSTM_LAYER_2')(lstm_layer)\n",
    "    lstm_layer = LSTM(32,return_sequences=True, name = 'LSTM_LAYER_3')(lstm_layer)\n",
    "    lstm_layer = Flatten(name = 'FLATTEN')(lstm_layer)\n",
    "    LSTM_combined = Concatenate(axis=1, name ='CONCAT')([lstm_layer,static_input])\n",
    "    output = Dense(1,activation='sigmoid',name='LSTM_OUTPUT_LAYER')(LSTM_combined)\n",
    "    model = Model(inputs=[temporal_input,static_input],outputs=[output])\n",
    "    return model\n",
    "\n",
    "def CNN_Concat_XGB():\n",
    "    cnn_layer = Conv1D(filters=4, kernel_size=5, activation='relu',padding='same', name=f'CNN_LAYER_1')(temporal_input)\n",
    "    cnn_layer = MaxPooling1D(pool_size=3)(cnn_layer)\n",
    "    cnn_layer = Conv1D(filters=8, kernel_size=3, activation='relu',padding='same', name=f'CNN_LAYER_2')(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Flatten(name='FLATTEN')(cnn_layer)\n",
    "    CNN_combined = Concatenate(axis=1, name='CONCAT')([cnn_layer, static_input])\n",
    "    output = Dense(1, activation='sigmoid', name='cnn_OUTPUT_LAYER')(CNN_combined)\n",
    "    model = Model(inputs=[temporal_input, static_input], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "temporal_input = Input(shape=(num_time_steps, num_features),name = 'TEMPORAL_INPUT')\n",
    "static_input = Input(shape=(X_train_static.shape[1]),name = 'STATIC_INPUT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and evaluating on test set for optimized configurations (concat - dense)\n",
    "\n",
    "def concat_dense_evaluation(X_train_temporal,X_train_static, y_train, X_test_temporal,\n",
    "                            X_test_static, y_test, batch_size, optimizer, initial_learning_rate, model):\n",
    "    \n",
    "    decay_rate = 0.1  # Decay rate\n",
    "    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "    epochs = 50\n",
    "\n",
    "    def learning_rate_scheduler(epoch):\n",
    "        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "    \n",
    "    if optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([X_train_temporal, X_train_static], y_train, epochs=epochs, batch_size=batch_size,\n",
    "                        validation_data=([X_test_temporal, X_test_static], y_test),\n",
    "                        shuffle=False,callbacks = [lr_scheduler])\n",
    "\n",
    "    # Plot the loss on train vs validate tests\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on the testing set\n",
    "    y_pred_probs = model.predict([X_test_temporal, X_test_static])\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    test_accuracy =  accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred)\n",
    "    test_recall = recall_score(y_test, y_pred)\n",
    "    test_f1 =  f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    y_train_pred_probs = model.predict([X_train_temporal, X_train_static])\n",
    "    y_train_pred = (y_train_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "   \n",
    "    print(f\"Testing Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % test_accuracy)\n",
    "    print(\"Precision: %.3f\" % test_precision)\n",
    "    print(\"Recall: %.3f\" % test_recall)\n",
    "    print(\"F1 score: %.3f\" % test_f1)\n",
    "  \n",
    "    print('-----------------------------------')\n",
    "    print(f\"Training Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Precision: %.3f\" % train_precision)\n",
    "    print(\"Recall: %.3f\" % train_recall)\n",
    "    print(\"F1 score: %.3f\" % train_f1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and evaluating on test set for optimized configurations (concat - ML)\n",
    "\n",
    "def concat_ML_evaluation(X_train_temporal,X_train_static, y_train, X_test_temporal, \n",
    "                         X_test_static, y_test, batch_size, optimizer, initial_learning_rate, model, ml_model):\n",
    "    \n",
    "    decay_rate = 0.1  # Decay rate\n",
    "    decay_steps = 20  # Decay steps (number of steps before applying decay)\n",
    "    epochs = 50\n",
    "\n",
    "    def learning_rate_scheduler(epoch):\n",
    "        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "    \n",
    "    if optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([X_train_temporal, X_train_static], y_train,\n",
    "                                                            epochs=epochs, batch_size=batch_size,\n",
    "                                                            validation_data=([X_test_temporal, X_test_static], y_test),\n",
    "                                                            shuffle=False, callbacks=[lr_scheduler])\n",
    "\n",
    "    # Plot the loss on train vs validate tests\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    preds,preds_train = classifier_prediction(X_train_temporal,X_test_temporal,X_train_static, \n",
    "                                              X_test_static,y_train,y_test, model = ml_model, \n",
    "                                              feature_extractor_model = model, layer_name='CONCAT')\n",
    "    \n",
    "    \n",
    "    # Evaluate on the testing set\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    test_precision = precision_score(y_test, preds)\n",
    "    test_recall = recall_score(y_test, preds)\n",
    "    test_f1 =  f1_score(y_test, preds)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    train_accuracy = accuracy_score(y_train, preds_train)\n",
    "    train_precision = precision_score(y_train, preds_train)\n",
    "    train_recall = recall_score(y_train, preds_train)\n",
    "    train_f1 = f1_score(y_train, preds_train)\n",
    "   \n",
    "    print(f\"Testing Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % test_accuracy)\n",
    "    print(\"Precision: %.3f\" % test_precision)\n",
    "    print(\"Recall: %.3f\" % test_recall)\n",
    "    print(\"F1 score: %.3f\" % test_f1)\n",
    "  \n",
    "    print('-----------------------------------')\n",
    "    print(f\"Training Set Performance\")\n",
    "    print(\"Accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Precision: %.3f\" % train_precision)\n",
    "    print(\"Recall: %.3f\" % train_recall)\n",
    "    print(\"F1 score: %.3f\" % train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dense_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static, \n",
    "                        y_test_preprocessed, batch_size=64, optimizer='adam',\n",
    "                        initial_learning_rate=0.0001, model=RNN_Concat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dense_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                        y_test_preprocessed, batch_size=64, optimizer='adam', \n",
    "                        initial_learning_rate=0.0001, model=LSTM_Concat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Concat (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dense_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                        y_test_preprocessed, batch_size=64, optimizer='adam', \n",
    "                        initial_learning_rate=0.001, model=CNN_Concat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                     y_test_preprocessed, batch_size=128, optimizer='adam',\n",
    "                     initial_learning_rate=0.0001, model=RNN_Concat_LR(), ml_model=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal,\n",
    "                     X_test_static, y_test_preprocessed, batch_size=32, optimizer='rmsprop', \n",
    "                     initial_learning_rate=0.0001, model=RNN_Concat_RF(), ml_model=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal,\n",
    "                     X_test_static, y_test_preprocessed, batch_size=128, optimizer='adam',\n",
    "                     initial_learning_rate=0.0001, model=RNN_Concat_XGB(), ml_model=XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                     y_test_preprocessed, batch_size=64, optimizer='adam', \n",
    "                     initial_learning_rate=0.0001, model=LSTM_Concat_LR(), ml_model=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal,\n",
    "                     X_test_static, y_test_preprocessed, batch_size=64, optimizer='adam',\n",
    "                     initial_learning_rate=0.001, model=LSTM_Concat_RF(), ml_model=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal,\n",
    "                     X_test_static, y_test_preprocessed, batch_size=64, optimizer='adam',\n",
    "                     initial_learning_rate=0.0001, model=LSTM_Concat_XGB(), ml_model=XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Concat + ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                     y_test_preprocessed, batch_size=32, optimizer='adam', \n",
    "                     initial_learning_rate=0.001, model=CNN_Concat_LR(), ml_model=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                     y_test_preprocessed, batch_size=64, optimizer='adam', \n",
    "                     initial_learning_rate=0.001, model=CNN_Concat_RF(), ml_model=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ML_evaluation(X_train_temporal,X_train_static, y_train_preprocessed, X_test_temporal, X_test_static,\n",
    "                     y_test_preprocessed, batch_size=64, optimizer='adam', \n",
    "                     initial_learning_rate=0.001, model=CNN_Concat_XGB(), ml_model=XGBClassifier())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
